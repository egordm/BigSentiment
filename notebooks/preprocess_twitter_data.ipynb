{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit to: https://www.kaggle.com/kyakovlev/preprocessing-bert-public\n",
    "\n",
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata, torch\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial vars"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "HELPER_PATH             = '../data/helpers/'\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "\n",
    "## Load helper helper))\n",
    "def load_helper_file(filename):\n",
    "    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n",
    "        temp_obj = pickle.load(f)\n",
    "    return temp_obj\n",
    "\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helpers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Multiprocessing Run.\n",
    "# :df - DataFrame to split                      # type: pandas DataFrame\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "def df_parallelize_run(df, func):\n",
    "    num_partitions, num_cores = 16, psutil.cpu_count()  # number of partitions and cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "\n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2)\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "## Domain Search\n",
    "re_3986_enhanced = re.compile(r\"\"\"\n",
    "        # Parse and capture RFC-3986 Generic URI components.\n",
    "        ^                                    # anchor to beginning of string\n",
    "        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n",
    "        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n",
    "             (?P<path>        [^?#\\s]*)      # capture required path\n",
    "        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n",
    "        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n",
    "        $                                    # anchor to end of string\n",
    "        \"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "re_domain =  re.compile(r\"\"\"\n",
    "        # Pick out top two levels of DNS domain from authority.\n",
    "        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n",
    "        (?::[0-9]*)?                      # Optional port number.\n",
    "        $                                 # Anchor to end of string.\n",
    "        \"\"\",\n",
    "        re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def domain_search(text):\n",
    "    try:\n",
    "        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n",
    "    except:\n",
    "        return 'url'\n",
    "\n",
    "## Preprocess helpers\n",
    "def place_hold(w):\n",
    "    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n",
    "\n",
    "def check_replace(w):\n",
    "    return not bool(re.search(WPLACEHOLDER, w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def make_dict_cleaning(s, w_dict):\n",
    "    if check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s\n",
    "\n",
    "def export_dict(temp_dict, serial_num):\n",
    "    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n",
    "\n",
    "def print_dict(temp_dict, n_items=10):\n",
    "    run = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        print(k,'---',v)\n",
    "        run +=1\n",
    "        if run==n_items:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get basic helper data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "good_cols       = ['_id', 'text']\n",
    "data = pd.read_csv('../data/bitcoin_twitter_raw.csv')\n",
    "if LOCAL_TEST:\n",
    "    data = data.iloc[:10000][good_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word / Vocab cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 35646 | Known words: 5919\n"
     ]
    }
   ],
   "source": [
    "texts = data['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "verbose = True\n",
    "global_lower=True\n",
    "texts = texts.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 29702 | Known words: 6942\n"
     ]
    }
   ],
   "source": [
    "if global_lower:\n",
    "    texts = texts.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 29514 | Known words: 6941\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "# Global\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "texts = texts.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "texts = texts.apply(lambda x: deaccent(x))\n",
    "if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 29514 | Known words: 6941\n"
     ]
    }
   ],
   "source": [
    "# Remove 'control' chars\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 29514 | Known words: 6941\n"
     ]
    }
   ],
   "source": [
    "# Remove hrefs\n",
    "# Global\n",
    "texts = texts.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 28072 | Known words: 6962\n",
      "ðŸ˜ˆâ¤µðŸ¾ðŸ˜§âœ”ðŸ”»ðŸ˜¤ðŸ”’ðŸ“¦â–¶ðŸ§¼ðŸ˜»ðŸšœâ €ðŸ’²ðŸ€ð˜¼ðŸ”¹ðŸ™ðŸ“£ðŸ”‹ðŸ˜˜ð’ðŸ“¢ðœð”ðŸŽðŸŽðŸ©ðŸ­ðŸ©³ðŸ“Šâ‹†ðŸ˜†ð‹â–ºðŸ¥¶ðŸ‡¨ðŸðŸ˜âš”â˜„â ðŸ“¹ðŸ‘°ðŸ’œðŸ„ðŸŽ„ðŸ‡­ðŸŒ¹æ¡ð«ðŸŽðŸ“°ðŸ¥²ðŸŸ¡ðŸ‡ºðŸ’·ðŸ’‡ðŸ’€ð™€ðŸ˜ŒðŸ›°ðŸ”œâ­•â†ªðŸŒ•ðŸ™ˆà¸¿ðšâ˜‚ðŸ‚ðŸ“ŒðŸ’³ðŸ¤“ðŸ—£âš–ðŸ”«ðŸŽŠðŸ¿â†—ìžðŸŒŽðŸ²â˜ âœ…ðŸ¾ðŸ¤–ðŸ¤¯ðŸŽŸì°¬ðŸ™ðŸ¥£ðŸ•ðŸ“©ðŸš‚ðŸ–ŠðŸ”žðŸŽˆðŸ”ðŸ‘‡ð…ðŸ¤«ðŸ‘ˆâ€¼ð­ððŸ¤¦ðŸ˜ƒðŸ“žæðŸ’ðŸŽžâ¬†ðŸ„ðŸ‡«ðŸ’¯ðˆðŸŒžéœœðŸŽ²ð˜¿ðŸ‡µðŸ’¬ðŸŒðŸš¨ðŸ˜­ðŸ¤£â›“â˜®ðŸ¦®â‚¿ðŸºðŸ’ðŸ’©à¸„ðŸ•µðŸ»ðŸ€ðŸ“ðŸ™†âš›ð™ðŸ¥ºðŸ™„ð¡ðŸ›©ðŸ‡¿ðŸ¥©ðŸŽðŸ¦„â¬‡ðŸ¤¾ðŸ†ðŸ…±ð§íŠ¸ðŸ˜ ðŸ¥•ðŸ‘±ðŸŽ™â¦ðŸ¤¸ðŸ˜œðŸŠðŸ˜‘ðŸ§¡ðŸ§®âš½í”¼ðŸ‘Œð€ð—¢ðŸš€ðŸ™ŒðŸŒ½ð™©ðŸ“·ðŸ˜½ðŸŽ¶ðŸš¶ðŸŒˆðŸŒ¸ðŸ‡³ðŸ™‹ðŸŒ‡ðŸ˜ðŸŽ°ðŸŒšðŸ–¤ðŸ’µðŸššðŸ ðŸ˜’âŒšðŸ’ªðŸ§ à¸°ðŸŒ”ðŸŽ‰ðŸŒ³ð—œðŸŽ§ðŸ’•ðŸ‡¦ðŸ•’ðŸ’ŠðŸ˜¯ð—•â™¾ðŸ’¼â˜ºðŸ˜…ðŸ’“ðŸ‘‘ðŸ’¸ë³¸ðŸ¥µðŸŸ ðŸ¤œðŸ¥§ðŸ½ðŸ˜žðŸ¤”ðŸ¦¢ðŸ’ ðŸ†™ðŸ˜™âš ðŸ‡¬ðŸ’ðŸª„â›”ç‰¹ç¹‹ðŸ’½ðŸŒ˜ð—§ð¢ðŸ¤‘ðŸ”¼ðŸ¦ðŸ‘‰ðŸ˜•ðŸ®ðŸ¦ŠðŸ¦ˆðŸ‘¸ðŸ’¡ðŸ˜‰ðŸ¦ðŸ˜€ðŸ©ðŸ”ð—”ðŸ¤¨ðŸ¶âš“ðŸ‘Žâ˜•â˜¯ì—´â™€í•´ðŸŒ¾â¤ðŸ»ðŸ´ðŸ§žð—˜ðŸ˜±ðŸ¥žðŸ‡¸ðŸ¤©âœŒï¿¼ð‡âš¾ë²•ðŸ“½ð—¦ðŸ‘ŸðŸ‚å‘ªð™ð‘ì¸ðŸ›¡ðŸŽ¯å»»ðŸ“ºâ—ðŸ›¸ðŸ§ªðŸ’‹ðŸ¥œðŸ¤®ðŸ™‚ð™žðŸ¦–ð™‰âœ‹ðŸ”™ðŸ”ð—¡ðŸ‰ðŸ¦¡ðŸ‘ðŸ—»â™‚â€ðŸ”˜ðŸ¦ºðŸ§ŽðŸ‘©â˜€ðŸ”ðŸ“ðŸ˜¶â°ðŸ’ŽðŸ’žðŸ¼â“ðŸŒ¶ðŸ’§âš•ðŸŽ…ðŸ’°ð™‘ðŸ”ŠðŸ’–ð–ðŸ‡·ðŸŽ®ðŸ”ŽâœˆðŸŒðŸ’±ðŸ®ðŸ˜ç€§ìµðŸ“¸ðŸ•¶ð™„ðŸŽ¢âš˜ðŸ¥³ðŸ˜ªï¹©ðŸ¤šðŸŽ†ðŸ†—ðŸ”¸âŒðŸ‘†âœŠðŸ“ðŸ˜ðŸ¥¸ðŸ’¨ðŸ§‘ðŸª™ðŠðŸ˜«æ‚Ÿâ‚¦ð™£ðŸ˜©ðŸ•ºðŸ”ðŸ‘„ðŸ’šâ›µðŸ‘¾ðŸ¦…ðŸ¥ðŸ¥·ðŸ˜¥ðŸ·ðŸ“®ðŸ“œð—â›·ðŸ¦ððŸ’«à¸”ðŸ¦ðŸ¤ðŸ”½ð ð—šðŸ¤¼ðŸ“•ð™Šð„ðŸ¥®ðŸ˜Žå¸ì •ðŸ‹ðŸ˜¼ðŸ’…ðŸ˜¨ðŸ˜„ðŸ“„ðŸ¤¤ðŸ‘ðŸ§ŠðŸ¥±ðŸŒ¼ðŸƒðŸ‡°è¡“ðŸ°é™ðŸ–±â©ðŸ•Šð‚ð— ð—Ÿâ†©ðŸŒŒå…ˆð—¥ðŸ¤™ðŸ»ë¹„ðŸðŸ§µâŸ¶ðŸ‘ðŸ’—ðŸ’ŒðŸŒ™ðŸ¥‚ðŸ˜²ðŸ˜´ë´ðŸ¤ªðŸ˜¹âœ¨ðŸ¤ŸðŸ”„ðŸ’¦ðŸ¿ðŸª¨ðŸŽ¤â¬…ðŸ“²ì™•ðŸ˜‚ðŸ‡¹ðŸ„â˜ŽðŸ’ðŸŒ‘ðŸ˜·ðŸ¤›ðŸ¤ ð™ŽðŸ›’ðŸ“’ðŸðŸ“ˆðŸ¥‹ð˜¾ðŸ¤ð“âž¡ðŸ’¥ðŸ’ƒðŸ˜ðŸ”šðŸ‘»âš™ðŸ¨ã€°ðŸ‘•ððŸ‰ðŸŸ©ðŸ˜”âƒ£ðŸ˜ŸðŸ†“â±ðŸ˜“ðŸ§ð¯ðŸ”¥ðŸ¥ªðŸ“‰ðŸ”´ð‰ðŸ™ŠðŸ’£â£ðŸ‘—ðŸª“ðŸŒ¿â˜¹ðŸ––ðŸŒœìŠ¤ðŸ”‘ðŸ‘¨ðŸ¥…ðŸŒ´ðŸ‘€ðŸ¹ðŸ§ðŸŒ€ðŸ˜‡ðŸŽ©ðŸ¦°ðŸ¤³ðŸ¤­ðƒðŒâ–“ðŽðŸ”®ðŸ¤·ðŸ‡©ðŸ‡§ðŸ“–ðŸ’”â›³ðŸ¤¡ðŸ˜®ðŸ‘âœë°ðŸ˜¡ðŸ’´ðŸ‘–ðŸ«ì½”ðŸ””ðŸ¬ðŸ¥¤ðŸ‡±â³ðŸŒðŸŽµðŸ³ð²ð†ðŸŒŠðŸ¤˜ðŸ¥´ðŸ”—ðŸ±ðŸ‡ªðŸ…ðŸ“…ðŸªðŸ“¶ðžðŸŽ£ðŸ˜Šâ­ðŸ‘£ð˜ðŸ˜³ðŸ‘…ðë¦¼ðŸ“¡ðŸŒ‹à¸ŠðŸ‘½ðŸŒ–ðŸ”ŒðŸ­â˜ðŸ¦‹âœ“ðŸ¤§ðŸŠðŸ¤žðŸ•—ðŸŒŸçµµðŸ’»ðŸ˜£ðŸ¤—ðŸ¦¾ðŸ•ð•ðŸ¥ŠðŸ¤•ë°©ðŸ˜‹ðŸ§ðŸ¡†â›âš¡ðŸ’­ðŸ§˜ìˆ˜ðŸ“±ðŸ–¼ð™ªðŸ’¹ð—£ðŸ¸ðŸ¤²ðŸâ–‘ðŸ˜¬ðŸ‘‹ðŸŽ±ðŸš«ðŸŒ›ðŸ¤ðŸŽ‚ðŸðŸ¥°ðŸ¤´ðŸ‡®ðŸ”¬ðŸªðŸ‘ðŸ¬ðŸ§ðŸ’¶ðŸ’†ð¨ðŸ˜šðŸ™ƒðŸ¦³ðŸ”¨ðŸ‘Šà¹„â˜ð™”ðŸ˜¢ðŸ”²\n",
      "128520 --- \n",
      "10549 --- \n",
      "127870 --- \n",
      "128551 --- \n",
      "10004 --- \n",
      "128315 --- \n",
      "128548 --- \n",
      "128274 --- \n",
      "128230 --- \n",
      "9654 --- \n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 28024 | Known words: 6953\n",
      "Â·Ñ‚à¸­æ¯”ç”ŸÐ½ì´ì‚¬à¸•à¹€â˜†ã€ÑÛŒÐµâ™¦à¸—äº”à¸™â™£ã„â—å‰â‡’Øªã¨à¸²ï¼ã‹â˜…Î¾æ˜ŸØ¨â€¦Ð·Ú©æ˜Žæˆ¦ãŠÐ²â€¢à¸¡Ñ‡â€žã€‚Ð±à¸¢Ù†Ùˆå°ãŸã•â™¥Ñ€Ð¾Ð°â‚¬ã‚Šãâ„¢à¸£ã‚“âˆšâ‰¥Ð¸Ðºã‚ˆÐ»\n",
      "183 --- \n",
      "1090 --- \n",
      "3629 --- \n",
      "27604 --- \n",
      "29983 --- \n",
      "1085 --- \n",
      "51060 --- i\n",
      "49324 --- \n",
      "3605 --- \n",
      "3648 --- e\n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars = 'Â·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 28024 | Known words: 6953\n"
     ]
    }
   ],
   "source": [
    "# Remove html tags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if ('<' in word) and ('>' in word):\n",
    "        for tag in html_tags:\n",
    "            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                temp_dict[word] = BeautifulSoup(word, 'html5lib').text\n",
    "texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 20306 | Known words: 6953\n",
      "https://t.co/r5kewo764x --- word_placeholder[t.co]\n",
      "https://t.co/ghvivaiu2l --- word_placeholder[t.co]\n",
      "https://t.co/89gomj1rdr --- word_placeholder[t.co]\n",
      "https://t.co/fnu8rqjii8 --- word_placeholder[t.co]\n",
      "https://t.co/sptecbzkgn --- word_placeholder[t.co]\n",
      "https://t.co/u09c9xnwxe --- word_placeholder[t.co]\n",
      "https://t.co/xbjftq74su --- word_placeholder[t.co]\n",
      "https://t.co/3zgq3qjmtc --- word_placeholder[t.co]\n",
      "https://t.co/uotxann5yl --- word_placeholder[t.co]\n",
      "https://t.co/b1h2vkqbad --- word_placeholder[t.co]\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it))\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "\n",
    "for word in temp_dict:\n",
    "    new_value = temp_dict[word]\n",
    "    if word.find('http')>2:\n",
    "        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n",
    "    else:\n",
    "        temp_dict[word] = place_hold(new_value)\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1.5:\n",
      "Unknown words: 20306 | Known words: 6953\n"
     ]
    }
   ],
   "source": [
    "# Remove twitter links\n",
    "temp_dict = {\n",
    "    'word_placeholder[t.co]': ''\n",
    "}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1.5:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 20306 | Known words: 6953\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "\n",
    "for word in temp_vocab:\n",
    "    url_check = False\n",
    "    if 'file:' in word:\n",
    "        url_check = True\n",
    "    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "        if 'Aww' not in word:\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone in word:\n",
    "                    url_check = True\n",
    "                    break\n",
    "    elif ('/' in word) and ('.' in word):\n",
    "        for d_zone in url_extensions:\n",
    "            if '.' + d_zone + '/' in word:\n",
    "                url_check = True\n",
    "                break\n",
    "\n",
    "    if url_check:\n",
    "        temp_dict[word] =  place_hold(domain_search(word))\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 20306 | Known words: 6953\n",
      ":))) --- ðŸ˜)\n",
      ":-) --- ðŸ˜\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if (pict in word) and (len(pict)>2):\n",
    "                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "            elif pict==word:\n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 20306 | Known words: 6953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate emoji\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print(chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 19246 | Known words: 6995\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "        if (Counter(word)['.']>1):\n",
    "            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "        if (Counter(word)['!']>1):\n",
    "            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "        if (Counter(word)['?']>1):\n",
    "            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "        if (Counter(word)[',']>1):\n",
    "            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 19244 | Known words: 6995\n",
      "___ --- \n",
      "#__ --- #\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "        temp_dict[word] = re.sub('_', '', word)\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 19240 | Known words: 6995\n",
      "************************************* ---  *   *   * \n",
      "$$$$ ---  $   $   $ \n",
      "#### ---  #   #   # \n",
      "$$$ ---  $   $   $ \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 19239 | Known words: 6995\n",
      ":( --- ðŸ˜¡\n",
      ";) --- ðŸ˜œ\n",
      ":) --- ðŸ˜\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if pict==word:\n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Isolate brakets and quotes\n",
    "# Global\n",
    "chars = '()[]{}<>\"'\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Break short words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 18865 | Known words: 7012\n",
      "cbn/okonjo-iweala/luno/ghana --- cbn / okonjo-iweala / luno / ghana\n",
      "monitoring/purchasing --- monitoring / purchasing\n",
      "/jonathan/gabriel/ozo ---  / jonathan / gabriel / ozo\n",
      "standard/professional --- standard / professional\n",
      "#blockchain_technology --- #blockchain technology\n",
      "misinterpretation/pseudo-analysis. --- misinterpretation / pseudo-analysis.\n",
      "#cryptocurrency_mass_adoption --- #cryptocurrency mass adoption\n",
      "nigeria/crypto/#bitcoin/piggyvest/endsars --- nigeria / crypto / #bitcoin / piggyvest / endsars\n",
      "eth-&gt;aave-&gt;eth. --- eth &gt;aave &gt;eth.\n",
      "#netunrealizedprofit/loss --- #netunrealizedprofit / loss\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '_' in word:\n",
    "        temp_dict[word] = re.sub('_', ' ', word)\n",
    "    elif '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    elif len(' '.join(word.split('-')).split())>2:\n",
    "        temp_dict[word] = re.sub('-', ' ', word)\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 19051 | Known words: 6995\n",
      "@founderflori, --- word_placeholder[@___founderflori] ,\n",
      "#ada --- word_placeholder[#___ada]\n",
      "#fxstrategy --- word_placeholder[#___fxstrategy]\n",
      "#cdwsocial --- word_placeholder[#___cdwsocial]\n",
      "@tap2crypto --- word_placeholder[@___tap2crypto]\n",
      "#bandusdt --- word_placeholder[#___bandusdt]\n",
      "@peterlbrandt --- word_placeholder[@___peterlbrandt]\n",
      "#belgium! --- word_placeholder[#___belgium] !\n",
      "#bitshares --- word_placeholder[#___bitshares]\n",
      "@80strolls --- word_placeholder[@___80strolls]\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n",
    "        if word[len(word)-1].isalnum():\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:])\n",
    "            elif word.startswith('u/'):\n",
    "                 new_word = place_hold('@' + ' ' + new_word[2:])\n",
    "            elif word.startswith('r/'):\n",
    "                 new_word = place_hold('#' + ' ' + new_word[2:])\n",
    "        else:\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n",
    "            elif word.startswith('u/'):\n",
    "                 new_word = place_hold('@' + ' ' + new_word[2:])\n",
    "            elif word.startswith('r/'):\n",
    "                 new_word = place_hold('#' + ' ' + new_word[2:])\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 18868 | Known words: 7012\n",
      "@kevin_cage_ --- @kevin_cage\n",
      "@official_jhay_ --- @official_jhay\n",
      "@smithie___ --- @smithie\n",
      "_their_ --- _their\n",
      "@_checkmatey_ --- @_checkmatey\n",
      "@chris_belcher_ --- @chris_belcher\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[len(word)-1]=='_':\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1]!='_':\n",
    "                new_word = word[:i]\n",
    "                temp_dict[word] = new_word\n",
    "                break\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 18867 | Known words: 7012\n",
      "_their --- their\n",
      "_hal9001 --- hal9001\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[0]=='_':\n",
    "        for i in range(len(word)):\n",
    "            if word[i]!='_':\n",
    "                new_word = word[i:]\n",
    "                temp_dict[word] = new_word\n",
    "                break\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 12563 | Known words: 7543\n",
      "get? --- get ?\n",
      "states' --- states '\n",
      "heed. --- heed .\n",
      "costs, --- costs ,\n",
      "bleeds, --- bleeds ,\n",
      "37,227.27$ --- 37,227.27 $\n",
      "faces. --- faces .\n",
      "management. --- management .\n",
      "meat. --- meat .\n",
      "continues. --- continues .\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word),0,-1):\n",
    "        if word[i-1].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 11215 | Known words: 7692\n",
      "$cl.wt --- $ cl.wt\n",
      "$eosup --- $ eosup\n",
      "\"exploring --- \" exploring\n",
      "$laho --- $ laho\n",
      "(ark --- ( ark\n",
      "$33.8k --- $ 33.8k\n",
      "$flr --- $ flr\n",
      "$dogecoin --- $ dogecoin\n",
      "&gt;500 --- & gt;500\n",
      "(dmbs --- ( dmbs\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 11215 | Known words: 7692\n",
      "h.i.m --- word_placeholder[him]\n",
      "r.i.p --- word_placeholder[rip]\n",
      "g.o.a.t --- word_placeholder[goat]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "            temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "        else:\n",
    "            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Contractions:\n",
      "Unknown words: 11213 | Known words: 7692\n",
      "i'd --- word_placeholder[i___would]\n",
      "haven't --- word_placeholder[have___not]\n",
      "how's --- word_placeholder[how___is]\n",
      "didn't --- word_placeholder[did___not]\n",
      "it'll --- word_placeholder[it___will]\n",
      "we're --- word_placeholder[we___are]\n",
      "that's --- word_placeholder[that___is]\n",
      "we'd --- word_placeholder[we___would]\n",
      "c'mon --- word_placeholder[c'mon]\n",
      "she's --- word_placeholder[she___is]\n"
     ]
    }
   ],
   "source": [
    "# Apply spellchecker for contractions\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if word in helper_contractions:\n",
    "        temp_dict[word] = place_hold(helper_contractions[word])\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove \"s:\n",
      "Unknown words: 11010 | Known words: 7707\n",
      "queen's --- queen\n",
      "street's --- street\n",
      "market's --- market\n",
      "world's --- world\n",
      "bloomberg's --- bloomberg\n",
      "haram's --- haram\n",
      "person's --- person\n",
      "ath's --- ath\n",
      "brother's --- brother\n",
      "microsoft's --- microsoft\n"
     ]
    }
   ],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 11010 | Known words: 7707\n"
     ]
    }
   ],
   "source": [
    "# Convert backslash\n",
    "# Global\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]\n",
    "temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Dup chars (with vocab check):\n",
      "Unknown words: 10776 | Known words: 7760\n",
      "2800 --- 280\n",
      "xvii --- xvi\n",
      "** --- *\n",
      "reff --- ref\n",
      "32000 --- 320\n",
      "49000 --- 490\n",
      "choosen --- chosen\n",
      "remmember --- remember\n",
      "30777 --- 307\n",
      "brooooo --- bro\n"
     ]
    }
   ],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!). TODO check fist against vocab?\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "temp_vocab_dup = []\n",
    "\n",
    "for word in temp_vocab:\n",
    "    if not word.isalpha():\n",
    "        continue\n",
    "    temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "temp_vocab_dup = set(temp_vocab_dup)\n",
    "temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "\n",
    "for word in temp_vocab:\n",
    "    new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "    if new_word in temp_vocab_dup:\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate numbers:\n",
      "Unknown words: 10776 | Known words: 7760\n",
      "180,000 --- word_placeholder[180___,___000]\n",
      "2024 --- word_placeholder[2024]\n",
      "33,534.57 --- word_placeholder[33___,___534___.___57]\n",
      "40590.99 --- word_placeholder[40590___.___99]\n",
      "682,875 --- word_placeholder[682___,___875]\n",
      "7.4 --- word_placeholder[7___.___4]\n",
      "5,471,598 --- word_placeholder[5___,___471___,___598]\n",
      "0.72 --- word_placeholder[0___.___72]\n",
      "0,04 --- word_placeholder[0___,___04]\n",
      "36616.9805 --- word_placeholder[36616___.___9805]\n"
     ]
    }
   ],
   "source": [
    "# Isolate numbers\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "        if re.compile('[0-9]').sub('', word) != word:\n",
    "            temp_dict[word] = word\n",
    "\n",
    "global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "temp_dict = {k:place_hold(make_cleaning(k,chars_dict)) for k in temp_dict}\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 10774 | Known words: 7760\n",
      "--=[ --- -=[\n",
      "--& --- -&\n",
      "]=-- --- ]=-\n",
      ";-- --- ;-\n",
      "transactions--innovate --- transactions-innovate\n",
      "-----------------& --- -&\n",
      "free--&gt --- free-&gt\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 10774 | Known words: 7760\n"
     ]
    }
   ],
   "source": [
    "# Try join word (Sloooow)\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "    if (new_word in local_vocab) and (len(new_word)>3):\n",
    "        temp_dict[word] = new_word\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 7639 | Known words: 8147\n",
      "ðŸ˜œ ---  ðŸ˜œ \n",
      "c'mon --- c ' mon\n",
      "cumhurbaskanÄ±istifa --- cumhurbaskan Ä± istifa\n",
      "ðŸ˜¡ ---  ðŸ˜¡ \n",
      "twitter.com --- twitter . com\n",
      "ðŸ˜ ---  ðŸ˜ \n",
      "erdoganÄ±nyanÄ±ndayÄ±z --- erdogan Ä± nyan Ä± nday Ä± z\n",
      "t.co --- t . co\n"
     ]
    }
   ],
   "source": [
    "# Try Split word\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 9972 | Known words: 7868\n",
      "k3nneth --- kenneth\n",
      "r10 --- rio\n",
      "10s --- ios\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion\n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = convert_leet(word)\n",
    "    if (new_word!=word):\n",
    "        if (len(word)>2) and (new_word in local_vocab):\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 7873 | Known words: 8075\n"
     ]
    }
   ],
   "source": [
    "# Open Holded words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (not check_replace(k))]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "texts = texts.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 7640 | Known words: 8146\n",
      "insiders --- insider\n",
      "intelligents --- intelligent\n",
      "alerts --- alert\n",
      "validations --- validation\n",
      "wednesdays --- wednesday\n",
      "powerplants --- powerplant\n",
      "tonights --- tonight\n",
      "surges --- surge\n",
      "welcomes --- welcome\n",
      "whats --- what\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "               _id  \\\n0     1.357811e+18   \n1     1.358008e+18   \n2     1.358061e+18   \n3     1.358010e+18   \n4     1.358021e+18   \n...            ...   \n9995  1.353849e+18   \n9996  1.353831e+18   \n9997  1.353847e+18   \n9998  1.353818e+18   \n9999  1.353839e+18   \n\n                                                                                                                                                                                                         text  \n0                   blockchains rely on fees to incentivize participation in the decentralized ecosystem . but to get mainstream adoption , we might have to do away with transaction costs altogether . t.co  \n1                                                                                                                                annual percentage yield ( apy ) # blockchain # cryptocurrency # bitcoin t.co  \n2                                                                                                                                     on a long enough timeline every asset looks flat against # bitcoin t.co  \n3                                                                                                                                                                   # bitcoin breaks $ 40k . . . again ! t.co  \n4                                                                                                                                 life gets cheaper on the # bitcoin standard . @ jclcapital @ crypto _ daily  \n...                                                                                                                                                                                                       ...  \n9995                                                                                                                                                             is # bitcoin setting up for this move ? t.co  \n9996  make the best of your circumstances ; focus on being grateful for the things you have . b $ gvt @ genesis _ vision b # altseason2021 # altcoins # bch # bitcoin # bnb # ethereum # dot # link # zrx ...  \n9997                                                                                                                         bitcoin price looks to resume bull cycle after rising above $ 34k t.co # bitcoin  \n9998                                                   check out this video . explains exactly what is happening with bitcoin right now ! ! ! for more trader insight please follow me # btc # bitcoin . t.co  \n9999                                                                                                                                                          psa : we are early keep buying corn . # bitcoin  \n\n[10000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.357811e+18</td>\n      <td>blockchains rely on fees to incentivize participation in the decentralized ecosystem . but to get mainstream adoption , we might have to do away with transaction costs altogether . t.co</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.358008e+18</td>\n      <td>annual percentage yield ( apy ) # blockchain # cryptocurrency # bitcoin t.co</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.358061e+18</td>\n      <td>on a long enough timeline every asset looks flat against # bitcoin t.co</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.358010e+18</td>\n      <td># bitcoin breaks $ 40k . . . again ! t.co</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.358021e+18</td>\n      <td>life gets cheaper on the # bitcoin standard . @ jclcapital @ crypto _ daily</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>1.353849e+18</td>\n      <td>is # bitcoin setting up for this move ? t.co</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>1.353831e+18</td>\n      <td>make the best of your circumstances ; focus on being grateful for the things you have . b $ gvt @ genesis _ vision b # altseason2021 # altcoins # bch # bitcoin # bnb # ethereum # dot # link # zrx ...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>1.353847e+18</td>\n      <td>bitcoin price looks to resume bull cycle after rising above $ 34k t.co # bitcoin</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>1.353818e+18</td>\n      <td>check out this video . explains exactly what is happening with bitcoin right now ! ! ! for more trader insight please follow me # btc # bitcoin . t.co</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>1.353839e+18</td>\n      <td>psa : we are early keep buying corn . # bitcoin</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = texts\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "if not LOCAL_TEST:\n",
    "    data['text'] = texts\n",
    "    data.to_csv('../data/bitcoin_twitter_processed.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}