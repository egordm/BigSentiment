{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Credit for some parts to: https://www.kaggle.com/kyakovlev/preprocessing-bert-public\n",
    "# Number extraction and hashtags is my baby\n",
    "\n",
    "# General imports|  \n",
    "import pandas as pd\n",
    "import re, warnings, pickle, itertools, emoji, unicodedata\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from utils.datasets import *\n",
    "from pandarallel import pandarallel\n",
    "import fasttext\n",
    "\n",
    "pandarallel.initialize()\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "## Initial vars\n",
    "\n",
    "HELPER_PATH             = '../../data/helpers/'\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "verbose = True\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "URL_TAG = '@URL'\n",
    "USER_TAG = '@USR'\n",
    "NUMBER_TAG = '@NUM'\n",
    "HASH_TAG = '@HTAG'\n",
    "CURRENCY_TAG = '@CURR'\n",
    "TIME_TAG = '@TIME'\n",
    "DATE_TAG = '@DATE'\n",
    "IMMUTABLES = [\n",
    "    WPLACEHOLDER,\n",
    "    URL_TAG, USER_TAG, NUMBER_TAG, HASH_TAG, CURRENCY_TAG,\n",
    "    TIME_TAG, DATE_TAG\n",
    "]\n",
    "\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Preprocess helpers\n",
    "def place_hold(w, tag=WPLACEHOLDER):\n",
    "    return tag + '[' + re.sub(' ', '___', w) + ']'\n",
    "\n",
    "## Helpers\n",
    "def check_replace(w):\n",
    "    return not bool(re.search('|'.join(IMMUTABLES), w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def make_dict_cleaning(s, w_dict, skip_check=False):\n",
    "    # Replaces a word using dict if it is mutable\n",
    "    if skip_check or check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Get basic helper data\n",
    "\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "helper_custom_synonyms     = load_helper_file('helper_custom_synonyms')\n",
    "helper_currency_synonyms     = load_helper_file('helper_currency_synonyms')\n",
    "helper_custom_general_synonyms     = load_helper_file('helper_custom_general_synonyms')\n",
    "emoji_dict = set(e for lang in emoji.UNICODE_EMOJI.values() for e in lang)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Load Data\n",
    "good_cols       = ['_id', 'text']\n",
    "data = pd.read_parquet('../../data/bitcoin_twitter_test_raw/part_0.parquet')\n",
    "data = data.iloc[:20000][good_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 75185 | Known words: 6991\n"
     ]
    }
   ],
   "source": [
    "## Start preprocessing\n",
    "texts = data['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "global_lower=True\n",
    "texts = texts.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 65753 | Known words: 8352\n"
     ]
    }
   ],
   "source": [
    "def lower(texts):\n",
    "    texts = texts.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "if global_lower:\n",
    "    texts = texts.pipe(lower)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 65087 | Known words: 8387\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "def normalize_chars(texts):\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "    texts = texts.apply(lambda x: deaccent(x))\n",
    "    if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 65087 | Known words: 8387\n"
     ]
    }
   ],
   "source": [
    "def remove_control_chars(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_control_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 65087 | Known words: 8387\n"
     ]
    }
   ],
   "source": [
    "def remove_hrefs(texts):\n",
    "    texts = texts.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "    if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_hrefs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 64638 | Known words: 8422\n",
      "â€»è‡ªà¤ åŽì–´æŽ¥åŠð“²å£‡æ“é¤â‘¡æžè²·å»¶ë¦½å‡¦çµŒæ§˜æ¬²í…ŒëŒ€çŽ²æ™¯ë² à¸°å€å¼ç­–æ€–é‚ªçœ¼é›»æ”»è¾›è©±è«‹æ¬§æ·»ç¥¨ä¸ºæ‚Ÿçš†ð’é€ƒâ™¡èª¿èˆ¬ð…æœ«ê³ ëŠ˜ë‚´ãƒ¤å¿˜é¡Œèª¬à¸šè¦³å°Šä½•ê²Œê²¨æŠ€æŠ—å‰°åºçªï½³ä¹±å ±ç¦æš‡è§¦åŽèµ·æ¡ï½€éŒ²ç¨¼è²¼æ™©åˆð¢çž¬åˆ¥å°å„ªç—›ð—¥æŽ©ã‡â€¸åŠ¹â‡¨åªÛµæ˜‡æ—ë¦¬à®£å™‚æã‚ƒå˜˜é‡å°å½“åœ¨å½»ç©Û´ð«ìƒå·¥ðŸ•ç³»è¾¼èˆˆå‰²âˆ€å½•ç²ê´€æºæŒð§ç—…ðŸ°çŸ¿æ°—å«ðŸ¬ï¼…ë³¸ã€†è¿”èª­å·±å°„å¼€æ·±ï¼œå¯Œâ™§ç¯„ë‹¤à¥‰â€æ—¢ä»•é£²ãƒŒæ’¤è¦–æ•°è¨Žà¸³æ¦‚ç¸¦ç¨Žç¸¾ì •à°¸æ”¾è¨¼è®°è‘—å¼•è¨˜ç«œí°æ˜”åº§éš†â—¡æ¯æ¸›ç§Â¯ç„æ‰¾å­˜é»’ä»–æ¹§ç‡ƒå”®æ¢°ç±³æ‚ªå¿™è‰°Ù£é›†ðŸ‡¼ð—µë³µè¨€ç·ç‚¹ê²°Û±Û²ï½Œì„ ð’Šé©ë³€å…‹é‹æ¤œå¢—ï½±å­©å‘Šæ³¨â‡©ç‰¹è‡­ð“¸ç•¥æ”¹é•çŽ‡à°¨æ˜“èµ¶ë®¤ë‹ˆç…½à®™å¼·ØŸâ–¡ç”³éŸ³æ‹¡ç„‰è²¯ë³´ìœ„æ…¢ç”šå¯çµ‚è´«è¿Žã€‘ç§‘è®ºð—¨ë§Œí’€ê²ƒæ³£ä¸ªåœ§à¸Šä¿ºì¼æŽ¡êµ­æ›é”ð“ªå•†å¯¼â“¢éœ‡å—ä»²æ›œå¸‚ç•ªè¾¿äº†é™ë©°æ•´ð˜†ðŸ‡¶æ ¼íŒâ¯ð¥å¯¾è™Žè€ƒà¸‚Ù¡é€€ð—žå³ï½¼ð“·ç¶™æ”«ç§â¬£å‹‰ï¼½ë½ä¸ƒï½·æŠ¼ì—ˆëª»ìžˆì†Œä½ŽðŸ‡¯à¸„æ¨™è·æ¡ˆè­¯è§£æ±ºðŸ‡³é™æ‰€åŸ‹ë‚¸æ¯Žå°±ï¾ƒè©°åˆ—ì„œíšŒð‘¶é¥®å–„æ…Œï¾„å‹ç‚ºë¹„è³ªå‘Ÿæ§‹à¸’å¥¹à§±å¿µà¸“ç¨‹éŠäº¿ï¾žâ™¤æ¸ˆåƒë£Œìœ¼çŸ¥ä¸‡æ¬ºçº¯í”¼æ˜Œéœ€ð˜‚ë¼ï½…è¦ð”‚ðŠë¨¸ë„¤å¯Ÿé ¼ìŠ¤èŽ·ç©«ç¢ºåº”å€¤ï¾›é­”ðŸä¸œè¡ã‚å°‚é™æ®é¨°è£•å€’ì‹¤å‰é‚„ï¼ì œÙ ð†åŸºåŽ³à¹ƒå©´å‰µð—¬çœ è­°ä¼åƒ¹ï½’âž¤æŽ§ðŸ‡¹à®‡å¯©ë¬´ç„¶é¡à¸”å¼¾å‚™å¸ï¼è´­ìˆéžðŸ‡»å¯é›°ï¾Šë°±èµšè¿½ë²ˆé’Ÿä¼´æ‰“ð’ì€æŽ¨å‡Œãƒà§°å‹§æ‡¸ð‘·ì™•âž¥æŠ‘æ°”à°°ì‹œå·¨æ¶¨ë¥ æ•™å°å…±à¸å–¶æ±‚â—”éŒ¢ðŸ‡¦ï½‰ä¼¸åº¦ç„¡å²¸å¯’ë‚˜í˜¸ë‹ë´ç­”è¾»â‚¦ç„¼è„³æ—©é«­å›ºæ…£æŠžå¤­à¸‹åƒ•å‚¬ð’„è£…ê¸‰åŠ¨çµ„ç›£å‹˜ì¡°éº»ð‘ë§ð—˜ð—¦ðŸ‡¬èªð®ç®¡å±ì•Œæ•…å¡ð—°æœŸð“­å¦„ð’‰æ–½ë”ï½ƒè²¸å£âƒ£å½¢à¸¯ä»Šì½”à°¤ë¥¼å„²ï¾€ð‘©ðžÚ¡åˆ©â–‘ðŸì»¤æ¯ç¯€é…ä½™èƒŒæ¿€ç­†å†éŸ¿åŠ‡æ»žï¼’è·ƒì˜¤ç‰©åŸŸå¢žè‡³é“¾äºˆä¾¡à¦˜åœå„„å…„å…†å‘¨è¨´ç¦è‰ë„ˆè®Šè¤„è«–æ“¬ì„¸æ˜¨è‚©ìƒ‰è¯èª²æ®ºæ¡åŠì›ç•™å®¢èµ„ï½¸ë“±åŒ…ð“°í•´é¡˜å¿Œð“µì•½æ¶ˆð“®åˆ‡ç…§à¤”ë°ç£¨ê¸°ä»¥å”¯â£¿ðŸ‡±ðŸ‡§ï¿¼ì°¬åŽšä½ðŸ‡ºðŸ‡¨ê¸´ä¿®ï¾‹é·²æµï¾Œç«¯å‹¢æº€Æ€å®¤å«Œå €ç­‹å£²ï¾—ð•é€£ç”±é€æ‰¶æ¥­ë“¤à¦«å”ï¼¼è©æƒ³è‚¯å´ì¸à¸ å¤®â¦à¤‡ìœ¨å¾‹ð²å€ŸæŠ½ð—›éŠ€å¤œä»¶å½±ëŒ“å²ë°›æŠ•å¾ˆç›´ä½œï½¯å¾è·¯é›¢æ—…èŠ¸æð„ç·šæ–‰ç´„ì£¼å„¿ìš©äº’ðŸŽìˆ˜ë ¤êµ´å¯†å•ç·´è¡†ä»·è¬›ç¶ºâ‚ºè¨±ãƒ¶æ‰•è·³ì—¬å…ˆä¸¦ð˜ƒå…¥å‡„å­—é™¢å¸Œç‹¬â€¥ç§»å¹¶çµ¡ðŸ±íç¹‹æ¥½èƒ½ç· í™©à°¦ë™å™´è²¬ï¾™ï½ãƒ¦æ‹‰ä»¬ð©è¨­ðšðŸ‡²ì›€Û·ìµè½°ìž¥ï½°ä¼‘ç´”ð’ä¸Žå¯§æ‹ è­¦â–“ï½¹ì‹¬åˆ¶ã‚½å®Œè¼ªì¢…ä¼¼ä¼ºé£›é’±í”ŒåŒ»è¢«ç´ è€Œå¸°æ­»ç›–è¿·è‚¡æš—ð¤ìœ í•˜í‚¹ðŸ²ç¿’ç¯‰ï¼•å¹…æŸ“æ‹›ê³¼æ¨£ç€å‘³éŠ˜ä¹™æˆ»å“¡æ…‹ðè©³è¿„åŒ–âŸ¶è«‡ð—²ð—¶åˆ¤ì• ç”£ê±°é›£à°‡å¦™à¸ŸçŽ°à°¬æ‰±ðï½—ï¾Žå—åºƒðŒà¸®è£ç®€ë¡œé£ðŸµížˆäº²ç•Œåº•ä¸¸à±é€šå†Šç ´ä¸¡â—é ˆä¿‚ê°™ðŸ‡­ç«™ï¼‘ï¾Ÿåœˆæ€å†¬âœ¦ð˜„ï¾˜å€¼çŒ®æ­³ð—¹ç†±èº‡ç´«ç€§æ‰æ–¯â™ªä¹—ë‹åŽ»å€™é ð‹è²©æ•—ê°„ç‰Œð—®Û³íž˜âŽŒâ ç™»ë˜ð¦å‘è§’ð—¯äº¤è­˜à¸ˆæ¯ð“±æ¦´ä½æ®‹ï¾ç·Šð—¿å¥¥è¨³éœœë„æ©Ÿï¼†å¤±èºŠð”à°Ÿà¹„à¹†æ’¸é¢ë“œë‹¬ðŸ‡µæå¤‰æŸ„æ ‡è©•ï½‹ðŸ“æ¹¾æˆ’ã‚©è²¨æº–í–¥å®Ÿè´¢æ‹’æ‹¾è³¼ï¼‹è™›åˆ¸å½«ç€šå¿œæ¿è¼©æ”¯å¾—ðŸ‡¾å£°ë‹¨ð‡ç·‘í™€ãƒ¨è›‡à°•å‘½ãçƒŸè€…è¯†ç©©æ±‡å®žä»½è³‡é¡žæŽ¢é‚£ï¼ƒè‰²å¤¢Ú˜å¶é™°ï¼žð­è‡´è¨—å·Û°â—‡è„±å¸é»„â‘¢æ³¢à¸–å°Žè¨ˆæŠœå¨è´¦æ ªà¸å®ˆè´åŽ†å††æœ€è¶…â—œå¤šé“æà°¯ï½ºç¥­ð—¼ï¿¥çŸ­é€±ë¡±è–„è¿‘ë°©ð é¨™ç½®æ¯’Ë˜ä¹°ìŠµèžë²•ð—¢å´©ð¬éš¾ìŠ¹à¸˜å›£å±Šï¼„ð—±èµ°èžç‰‡åž‹íŠ¸ç½ªð“¬ä½¿ç‹™ï¼–å¸ƒè˜­æ™®åˆè¿å´æœ›å±¤èŒ¶æ”¶å¾©ï¼—å‰Šâ‚¿æŠ±é¹¿à¸œè»½ä»»è¾ºç§°ë‚œè¿‡å¼„æ•£à°£ï½²é™¤ä¸é¡†ç ²é ƒç¾à°‚å …é€²à¸›ë¹¼â˜žì—æ›´é¤¨ë§ˆåƒèª°æŠµæŒ–ä½ ð‘¿ì•”ì†ð—§ð˜ðŸ‡¸å´ðŽæ®µâ–¸æ¥µæ¤ï¼»ìžé¦–â©å–ð¡ð˜€à¹‚æŠ¥èŽæ‚©å–°å°†è½¦é€ã‚‡å–œí„°ð€ë§¤æ€¥ä¾‹Û¹è¼‰ðŸ‡ªé…®å……ê³„é›²é †è¶³æš´å¥½éº—ìš”å”ê¸ˆì €ç´¯ç†åˆ°ìš´å›æ€§é¡”æ¬¡â…’æœæŸ¥ë¥¸ð“½æ„Ÿç¿»å§‹çº¦ã€æ­´ðƒï½æèµ¤éŸ“ä½“é…¬å®ð˜å¸¯èª•éŽ–ï¼“ï¼¾ð¨ðï¼™ð—¡ç¶šçµ¦å°‘ðŸ‡½é–‹åœºçŠ¶å‡ë°é¬¼â—‹å„å—æ¯›å¦ð’•à°¡è±ªåŸƒå‰¯ç›Šç”»æ³›ä¼ç¹”éŽæ•·åŠåºœð“•ð‚å…¶é´ë¶€ì—´ë„·â–²ë°˜å¸¸å‹Ÿê·¸å›é€†â“œå“å›²ç¨¿çˆ†ðŸ‡°æµ®ð—»äºï¾”èƒ¸âœ§å·®â€Œå¡¾à°ªæ¸¬â–¼ç¬‘ì¹œè´§åˆºè¡“ì§€ç´¹í¬å¾“å½¼â €æ—¦ï¼›ðŸ‡´ðŸ‡¿í›„è¬ð“»ðˆð‘¹å‚å»ºæƒ‘æ¡â‚³è±Ší°è²»ç”¨à¸«é‡ä¾¿è²¿åˆ›ì±„ð’…æ€æœªâ–³å®ƒðŸ‡«ëŸ¬è²°æ³å› ç–‘çµè¡¨éœ²ç¬¬è±¡ç°¡ì•ˆë°°ðœç¨®å¹£ì—†Ùªç§©ð“å±•ï¾é™†ç¾¨é€Ÿå†·å‚¾ì„å‡‰åŠŸå›žì–µê°€ç–²æŸ»ä¹‰à°®æžœå‘ªæ—¶ðŸ‡®é ‘è‚‰ðŸ—éŒ¬ð—Ÿè² æ€€å»»é©¬çª“çµµæˆ·ð— ì‹­ç‚®í™”åŠ¡ç´â–ˆä»˜é¨“å˜æ´»ð’‚å‹‡æ„æ‹æŠ˜à®†ë‰´æ–­è¶Šå¿…æ¥ä¾›æŒ‡ç å¬‰í‹°åœ³æžšæŽ˜æ›¿æºè»¢æ™‚âœ“ðŸ‡·ë¦¼ì „åŠ´æƒ…é›€çµ¶å…¨è¦‡æ‹³æ˜¯å±€ëž˜æ‚²é–¢â‹†å¾®ä¾†å‹•ç¹°æŒ¯ë¬¼ä¼˜à¤‘æŽ’å…œæ²¡ã€…åœé¸à¹åŒ¿è¦æ¨¡ð“¼å¾…åŸ·ðŸ­ç´°æ—§å€‹è©¦é¤Šåˆ»â–ºà¸¿â‘ åå­¸æ¸¯è½ï¿½æ¨©ëŠ”ðŸç¥ðŸ‡©ë…œì•„æ–™ç®—ìŠ¬\n",
      "8251 --- \n",
      "33258 --- \n",
      "2336 --- \n",
      "21454 --- \n",
      "50612 --- \n",
      "25509 --- \n",
      "21322 --- \n",
      "120050 --- i\n",
      "22727 --- \n",
      "25805 --- \n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "def convert_remove_bad_symbols(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji_dict) and (c not in white_list_chars)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 62759 | Known words: 8380\n",
      "Â·Î²ÏƒåŠ à¸™ã€‚Øªæ°‘â‚±ã¸ç›®ãµ×©ä»®å‰åœ°ç¤º×¡ãƒ•ã€é¢¨é‡ŽÏ‰æ°ç«à¤·å®£å±±é«˜à¤¤à¦¡ã¤ãƒƒç€¬â†Ñ‚å…‰ãˆâˆšà¦¦ãƒ¯ã›é‡‘ã¬ã€ŽÎ¿ã‚¦ÑŒå°ãƒžä¸‹é•·à¤–×“æ›¸ÛŒçŸ³ã‚‚å­¦ã‹××›ä¸‰ã‚çœŸØ´Ï†Î¹à¤¿Øºà§€æˆ¦æ–‡äº¬ç›¸å ´ç”Ÿà¸²Ø³×¦à¤¶Ð¿Ïãƒ¬ãâ‡’ã€â˜…×™å››å¥³åŒºè¦‹Ñ€Ñ„ãŠãƒ„ã‚·Ø°ä¸ŠãŸã‚ªì´Ð¸à¸­æ˜¥Ñ‰Î³à¦¬ç«‹ãƒ§ä¿¡â€¢çš„à¸—×•ã€Œå¹¸äº•Ñ‡Ð³ï¼‰à¤¹Ø·áƒšà¥€Ø¡×ªã‚¢Ù¾à¤šÎºà®³â€¦æˆÎ·à¸‡ÑÐ¼à¦¯ã‚“ãƒ£å—Ù€æ–°ç™½Ð»ã‚±è‰¯à¥‹ã“Î½ã‚„Ð¾Ø®ãƒ¥à®°ãƒŽæ¯”à®µèªžåˆ†è¯à¤µé£Ÿà¤¨ãƒ¼ã¿Ï…ã‚ï¼ã‚¿à¦ªã‚£à¦®à¤œ×Ÿà¤¬æœ¬â†’à¸¡Ð°ã‚«Ð¶Ù‚Ø¹ï¼ˆÑˆì˜×Ñ‹×”Ø¯ÐµÏ„è¡Œå›½à®²à¦¸å°‡Ø§èŠ±à¤§ï¼ŒØ²à¯â‚¬å£«ã‚¹Ú†Ø¶å…ƒã‚‰æœãƒŸØ¬å­â”€å†…å¹³×šå¹´åŠ›Î¶×žä¸–à¸žæœˆç¥žÑƒåœŸà¦¤ãƒ‹Ø¨å¤–ã‚€ÐºÚ¯à®¿Ø«è‹±ÑŽà¤Ù‰â†‘à¦•à¸§à®¨à®¤Ñã‚³ì‚¬ã²×¢à¤—à¦šà¤¡ã‚¡å¥ˆå¤§ãƒä¿æœ‰Ø©Ù†ä»£æ­£ç”·æ”¿à®ŸÐ±â—ï¼šÑ†à¸ªÎµâ‚¹äººã‚¤é¦¬à¤°ç©ºà¸¢×¨ååƒà§‡×˜æˆ‘å¤©ã‚‹ãƒ»à¦ŸåŒè»Šå¼µä¼šÏ‡â˜†ãƒ«Ð·ä¹‹Î¸æœ¨ãƒâ†“ï¼ŸÑ…ã¯Ï‚à®šãƒŠäº”×œåŽŸå®šÙ…äº‹ãªä¹…äºŒã‚Šç™ºØ­ãƒ˜Î»ãƒ©åŒ—ãƒ­æ‰‹Î¾ã«ã‚¨Î±ØŒà¤¸à¤‰à¯‡å‡º×£×’ã‚»Ùˆå²æ˜Žãƒ›é™½ã†ç¤¾è°·à¥¤ãƒ å®‰ã‚ã¨å¿ƒéƒ¨Ùç¾©à®ªØ±ÙŠØ¸Ð½à¤®Ð²â€žéƒ½ã‚†à¸£ï¼ã‚Œã™à¸¥å£ä¹Ÿ×–à¤ªÐ´å’Œã¡ã‚’æ­¢ã‚­åˆä¸­à¤²åå®¶à¸å¹¿å¾Œãƒˆà¦¹ãƒ³ãƒªÎ¼×‘ã£ï½žà¦¾à¸•à®¾ã»ä¸æ˜Ÿããƒ’à¤¾×§ã‘ã®à¤¦à¤Ÿå³¶ã¦ç¾Žà¦²ã„è²´é“æ³•ã—ãƒ¢æ—¥ã‚§ãƒ¡à¤…ã•à®®à§‹à®•Ñå·žÏ€ã‚µãƒ†Î´ã€œä¸€å¤ÙƒØµã­à¤¯í•œé¢Ù„à¤†é¦™ä¸»à¹€çŠ¬ã¾æ–¹×—ã‚ˆà¦¿å‹Ù‡ã€ãæ°´é–€é–“à¦­çŽ‹à¦¨æµ·ä»‹Ú©à¤•â‰ˆã‚¯×¤å…¬× \n",
      "183 --- \n",
      "946 --- \n",
      "963 --- \n",
      "21152 --- \n",
      "3609 --- \n",
      "12290 --- \n",
      "1578 --- \n",
      "27665 --- \n",
      "8369 --- \n",
      "12408 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "def convert_remove_bad_symbols2(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = 'Â·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji_dict) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 62759 | Known words: 8380\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if ('<' in word) and ('>' in word):\n",
    "            for tag in html_tags:\n",
    "                if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                    temp_dict[word] = BeautifulSoup(word, 'html5lib').text\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_html_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 51806 | Known words: 8380\n",
      "https://t.co/gelqggviyj --- @URL[t.co]\n",
      "https://t.co/cwogs38kcz --- @URL[t.co]\n",
      "https://t.co/mcvchqq4bu --- @URL[t.co]\n",
      "https://t.co/mgck6u5sfi --- @URL[t.co]\n",
      "https://t.co/zmcxs8ygxz --- @URL[t.co]\n",
      "https://t.co/r1cjnu7xqe --- @URL[t.co]\n",
      "https://t.co/rmekxunkpn --- @URL[t.co]\n",
      "https://t.co/zmcr4o0mps --- @URL[t.co]\n",
      "https://t.co/bfhipl2exl --- @URL[t.co]\n",
      "https://t.co/qjwvxvjscq --- @URL[t.co]\n",
      "########## Step - Convert urls part 1.5:\n",
      "Unknown words: 51805 | Known words: 8380\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it))\n",
    "def remove_links(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "    temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "\n",
    "    for word in temp_dict:\n",
    "        new_value = temp_dict[word]\n",
    "        if word.find('http')>2:\n",
    "            temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value, URL_TAG)\n",
    "        else:\n",
    "            temp_dict[word] = place_hold(new_value, URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "\n",
    "    # Remove twitter urls\n",
    "    temp_dict = {\n",
    "        f'{URL_TAG}[t.co]': ''\n",
    "    }\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1.5:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove escaped html:\n",
      "Unknown words: 51757 | Known words: 8380\n",
      "==&gt;&gt; --- ==\n",
      "s&amp;p --- s and p\n",
      "stakepoolðŸ¥©&amp;ðŸ³ --- stakepoolðŸ¥© and ðŸ³\n",
      "&gt;&gt;&gt;&gt;&gt; --- \n",
      "love&gt;money. --- lovemoney.\n",
      "&lt;$0.20. --- $0.20.\n",
      "p&amp;d --- p and d\n",
      "&gt;remember --- remember\n",
      "&gt;&gt;7 --- 7\n",
      "&lt;$f/s --- $f/s\n"
     ]
    }
   ],
   "source": [
    "# Remove escaped html\n",
    "def remove_escaped_html(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    symbols = {\n",
    "        '&quot;': '',\n",
    "        '&amp;': ' and ',\n",
    "        '&lt;': '',\n",
    "        '&gt;': '',\n",
    "    }\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if any([rep in word for rep in symbols.keys()]):\n",
    "            new_word = word\n",
    "            for rep, to in symbols.items():\n",
    "                new_word = new_word.replace(rep, to)\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove escaped html:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_escaped_html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 51754 | Known words: 8380\n",
      ".io/boxes/all?r=5f4c54b0bd312243977db0f7 --- @URL[url]\n",
      "httpss://betfury.io/boxes/all?r=601593e4b08af17cbc468064 --- @URL[betfury.io]\n",
      "35%/betfury.io/boxes/all?r=600b1be208cc0b1e47440365 --- @URL[url]\n",
      "www.studio192.nle --- @URL[studio192.nle]\n",
      "//t.co/nf0x22os7q --- @URL[url]\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "def convert_urls2(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        url_check = False\n",
    "        if 'file:' in word:\n",
    "            url_check = True\n",
    "        elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "            if 'Aww' not in word:\n",
    "                for d_zone in url_extensions:\n",
    "                    if '.' + d_zone in word:\n",
    "                        url_check = True\n",
    "                        break\n",
    "        elif ('/' in word) and ('.' in word):\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone + '/' in word:\n",
    "                    url_check = True\n",
    "                    break\n",
    "\n",
    "        if url_check:\n",
    "            temp_dict[word] =  place_hold(domain_search(word), URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_urls2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 51752 | Known words: 8380\n",
      ":-) --- ðŸ˜\n",
      ":)) --- ðŸ˜\n",
      ":))) --- ðŸ˜)\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if (pict in word) and (len(pict)>2):\n",
    "                    char_pict = pict[-1].isalpha() and pict[0].isalpha()\n",
    "                    if char_pict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "                elif pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 50158 | Known words: 8404\n",
      "ðŸ©ºðŸ§ðŸ”»ðŸ“¨ðŸ•´ðŸºðŸ”ƒðŸŒ¼âš ðŸ›â¬†ðŸ±ðŸ’™ðŸ“ƒðŸŽ¢ðŸ’žðŸ˜‡ðŸ‹ðŸŽ¾â‰âš™ðŸ“ðŸŒ½â—ðŸ¤ðŸ˜ðŸŒªâ˜ºâ˜â£ðŸª„ðŸƒðŸ“ðŸ‘†â—»ðŸ˜ŠðŸ¾ðŸ¡ðŸ˜·â›³ðŸ¶ðŸŒ ðŸ”´ðŸ”ŸðŸª“â†•â˜¹ðŸŒ‚ðŸŽŠðŸ’ŒðŸ¤ŽðŸ¤ŒðŸŒµðŸ‘ðŸ‘ðŸ’¥ðŸ ðŸ›©ðŸ’µðŸŽ¬ðŸ”‹ðŸ™ŒðŸ–¨ðŸ”ðŸš¬â¬›ðŸ•’ðŸ‘ŠðŸŠðŸ‘¹ðŸ˜µðŸ––ðŸ™‹ðŸ’ðŸ¥‡ðŸ¥µâ—¼ðŸ•â–ªðŸ§¨ðŸ‘‹ðŸ”‰ðŸ’–ðŸ˜™ðŸŽ‡âœ”ðŸ•˜ðŸ¤²ðŸ’»âœ³ðŸŽðŸ›¥ðŸ›¢ðŸ¥·ðŸ–¥ðŸŒ›ðŸ§ðŸ–¼ðŸ’˜ðŸš¤â²ðŸš©ðŸ‘¢ðŸ¦¥ðŸ˜›ðŸ”’ðŸª€ðŸš¶ðŸ¥´ðŸ¤®ðŸ˜»ðŸ€ðŸ§™ðŸ°ðŸ›’â˜•â—¾ðŸ¥ˆâ˜‘ðŸ¤ ðŸ“¹ðŸ’…â¬…ðŸ”ŽðŸ¯ðŸŒ¸ðŸš€ðŸ˜³â˜€ðŸš¦ðŸ“¥âš«ðŸ¤­ðŸŸ âŒðŸ“±ðŸŸ©ðŸ“©ðŸ¤™ðŸ”¶ðŸ¥šðŸ¤ªðŸ“‰ðŸ’¬ðŸŽ¶ðŸ˜ðŸªœðŸ“¤ðŸ˜¹â›½ðŸ”¹ðŸŒ‰ðŸ’±ðŸ“šðŸŽ¥ðŸƒðŸ¤¢ðŸ›ðŸ’€ðŸ†™ðŸŒ§ðŸ˜¸â°ðŸ˜ŽâŒšðŸ˜²ðŸ¤¹â˜®ðŸŒŽðŸ§‘ðŸŒºðŸ’ðŸ‡ðŸ³âœŒðŸ‘©ðŸ˜ðŸ‘‰ðŸ§³ðŸ§±ðŸ•ðŸ–‡ðŸ˜§ðŸ˜˜ðŸ¾ðŸ–ðŸ‘ðŸ¤¨ðŸ˜¤ðŸ”ŠðŸ˜’ðŸŸ§ðŸ’¯ðŸ™†ðŸ¤‘âœˆðŸ¤¯â›‘ðŸŒÂ®ðŸ¤—ðŸ™€ðŸŒ±ðŸ¾ðŸ¦‰ðŸ™ˆðŸ¤šðŸŒ’ðŸ§­ðŸ”®ðŸ†ðŸŒ™ðŸ‘ºâ©ðŸ¥³ðŸ›¶ðŸ…°ðŸ™‚ðŸ¥‚ðŸâš–ðŸ‘ðŸ’¹ðŸ¤ŸðŸ­ðŸ»ðŸŽ†ðŸ’ƒâ¤µðŸ’°ðŸ“¡âœ–ðŸ˜„ðŸŒ¹ðŸ“…ðŸ†ðŸ”ðŸ•·ðŸ¼ðŸðŸ§ ðŸ¤¡ðŸŸ¢ðŸ‘¨ðŸˆðŸ§˜ðŸ‘ˆðŸŽ™ðŸŒ•ðŸ“€â˜”ðŸ—“ðŸ’¤ðŸŒ˜ðŸ‹ðŸ˜¶ðŸ”›ðŸðŸŽ€âž¡ðŸ†˜â±âœ‹ðŸ¿ðŸ•°ðŸ“·ðŸ¢ðŸ—£ðŸ’“ðŸ’ŠðŸ˜¢ðŸ½ðŸ¯ðŸ¤¬â˜ƒðŸŽðŸ’¡ðŸ’Ÿâ„ðŸš¨ðŸ“¢ðŸ”œðŸ€ðŸ†•â˜ðŸ§ŠðŸŽ§ðŸŽ‰â›”ðŸŒðŸ˜ðŸ‚ðŸ„ðŸŠâ›ðŸ“—ðŸ¥‰â–«â›„ðŸ­ðŸ‘ŒðŸŽ¯ðŸŸ¡ðŸ¥™â¬‡ðŸ¬ðŸ˜¯ðŸŒ‹ã€½ðŸ’­ðŸ“¯ðŸ”„ðŸ¦³â›µðŸ’¶ðŸ’©ðŸ‘§ðŸ’³ðŸ˜­â€¼ðŸ›³ðŸ©³ðŸ»ðŸ‘ŸðŸš‚Â©ðŸ¤ºðŸ’•ðŸ‘¦ðŸŒšðŸŒðŸšªðŸ“½ðŸ–ŒðŸ‘“ðŸ™ŠðŸƒðŸ“‹ðŸ§¡ðŸ“£ðŸ¤¾ðŸ’¦ðŸ•¯ðŸ’²ðŸ—¼ðŸ›«ðŸºðŸ™‰ðŸ¦„ðŸ¥©ðŸžðŸ¤¸ðŸ”¦â™¾â•ðŸ˜–ðŸ¤žðŸ”«ðŸŽ‚â™»ðŸ”˜ðŸ›ðŸŒ—ðŸ˜°ðŸ´ðŸŒðŸ‘½ðŸ“¸ðŸ’§ðŸ»ðŸª™ðŸ›‘ðŸðŸ“²ðŸ“¬ðŸ˜‰ðŸŽ¸ðŸ“ðŸŽ±ðŸ¤–ðŸ“¦â¬ðŸ¤£ðŸ¥ºðŸŒ´ðŸ¦ðŸ˜¨â™¥ðŸŒœâ„¹â³ðŸðŸ¦ƒðŸ’‹ðŸ”µðŸŸ¨ðŸŒ–ðŸ‘¤ðŸ˜‹â˜ ðŸ˜«ðŸ¤›ðŸ“°ãŠ—ðŸ’›ðŸ“ŒâœðŸ¸ðŸŽˆðŸ’´ðŸ¦ðŸš›ðŸ˜±âœŠðŸðŸ¦†ðŸ¤·ðŸ˜ˆâ›·âŒ›ðŸ¥ƒâœ…ðŸ–•â¤´ðŸ‘›ðŸ€ðŸ¤˜ðŸ†’ðŸŒŸðŸ§¬ðŸƒðŸ¿ðŸ“ðŸ™„ðŸ„ðŸ”—ðŸŒðŸ¥°ðŸ—½ðŸŒ®ðŸ¥¶â˜„â™¦ðŸ‘ðŸ‘…ðŸ™ðŸŽðŸ˜ªðŸ—žâ˜£â–¶ðŸ”·ðŸ’—ðŸ”°ðŸŒ‡ðŸ˜¬ðŸ¤«ðŸ·ðŸ‡ðŸœâ„¢ðŸ°â™€ðŸ™‡ðŸ’«âš›ðŸš˜ðŸ”½ðŸ¥ðŸŒ„ðŸ“ˆðŸ˜•ðŸŒ‘ðŸ’ðŸ¤´ðŸ“ðŸŠðŸ•ðŸ”ðŸ¦ ðŸ¬ðŸ˜®ðŸŒ…â¤ðŸª¨ðŸ¤ðŸ¦¾ðŸ…ðŸ‡âœ¨ðŸŒˆðŸ’ªðŸ‘ðŸŽðŸ”¸ðŸ¦°ðŸšŒã€°ðŸªðŸ›¸ðŸ“ªðŸŒ¡ðŸ†“ðŸŽ»ðŸŒžðŸ‘‡ðŸ™ðŸ³ðŸ˜´ðŸŽðŸ°ðŸ“†ðŸ’£ðŸ¤¦ðŸ•¹ðŸ‘‚ðŸ”ðŸ¤“ðŸŒŠðŸ¤¤ðŸ’ðŸ’œâ›´â™‚ðŸ˜€ðŸ¥žðŸ§ðŸ›ðŸ¥±ðŸ’ ðŸ˜‘ðŸ¤â™£ðŸ¤”ðŸ˜“ðŸš—ðŸŸðŸ˜¥ðŸ“ŠðŸ©¸ðŸ’¿ðŸ’¨ðŸ˜£ðŸ‘‘ðŸ’‰ðŸ‘”ðŸ’”ðŸŽ¼ðŸ¦ðŸ§²âš¡ðŸ–ðŸ¤§ðŸ˜ƒâ“ðŸ§¯ðŸ˜ŒðŸªðŸ”¥ðŸŒ”ðŸ¥²ðŸ–ðŸ¦â™ ðŸ§¿ðŸ•¸ðŸŽ¦ðŸ®ðŸ¦…ðŸ”¼â˜˜ðŸ˜…ðŸ˜”ðŸ”–ðŸ“„ðŸ‘ŽðŸŒðŸ› ðŸ¤©ðŸ’šðŸ™ƒâ­ðŸ”ºðŸ””ðŸ®ðŸ…±ðŸ§»ðŸ›°ðŸ˜ðŸ’Žâ‡ðŸ˜¡ðŸ’¸ðŸ•šðŸ˜ðŸŽ¨â›“ðŸ˜œðŸŒ“ðŸ‘€ðŸŽ¤ðŸ¤³ðŸ”žðŸ˜½ðŸ›¡ðŸ“ºðŸš„ðŸ–¤ðŸŒŒðŸŒðŸ˜‚ðŸ¦œðŸ˜šðŸ—¯âš½ðŸ˜†ðŸ•µðŸ›ŽðŸŽ£â­•\n"
     ]
    }
   ],
   "source": [
    "def isolate_emoji(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if c in emoji_dict])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 48775 | Known words: 8452\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "def deduplicate_dots(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "            if (Counter(word)['.']>1):\n",
    "                new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "            if (Counter(word)['!']>1):\n",
    "                new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "            if (Counter(word)['?']>1):\n",
    "                new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "            if (Counter(word)[',']>1):\n",
    "                new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(deduplicate_dots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 48762 | Known words: 8452\n",
      "#_ --- #\n",
      "__________________ --- \n",
      "___! --- !\n",
      "@l_____l____l___ --- @lll\n",
      "____ --- \n",
      "_______ --- \n",
      "_____ --- \n",
      "______.\" --- .\"\n",
      "webd____________________ --- webd\n",
      "#_l --- #l\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "def remove_underscore_spam(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "            temp_dict[word] = re.sub('_', '', word)\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_underscore_spam)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 48752 | Known words: 8452\n",
      "$$$$$ ---  $ \n",
      "*** ---  * \n",
      "**** ---  * \n",
      "$$$ ---  $ \n",
      "************** ---  * \n",
      "^^^^ ---  ^ \n",
      "$$$$ ---  $ \n",
      "*************** ---  * \n",
      "^^^^^ ---  ^ \n",
      "^^^ ---  ^ \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "def isolate_spam_characters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "            temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(1)])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_spam_characters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 48746 | Known words: 8452\n",
      ":) --- ðŸ˜\n",
      "=) --- ðŸ˜\n",
      ":( --- ðŸ˜¡\n",
      ";) --- ðŸ˜œ\n",
      "%) --- ðŸ˜µ\n",
      ":/ --- ðŸ¤”\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 46987 | Known words: 8523\n",
      "40 ---  ( \n",
      "41 ---  ) \n",
      "91 ---  [ \n",
      "93 ---  ] \n",
      "123 ---  { \n",
      "125 ---  } \n",
      "60 ---  < \n",
      "62 ---  > \n",
      "34 ---  \" \n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "def isolate_brackets(texts):\n",
    "    chars = '()[]{}<>\"'\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_brackets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Extract date and time:\n",
      "Unknown words: 46989 | Known words: 8523\n",
      "11:12:43 ---  @TIME[11:12:43] \n",
      "06:26:31 ---  @TIME[06:26:31] \n",
      "17:05:00 ---  @TIME[17:05:00] \n",
      "02/01/2021, ---  @DATE[02/01/2021] \n",
      "23:12:20 ---  @TIME[23:12:20] \n",
      "02/02/2021 ---  @DATE[02/02/2021] \n",
      "13:17:53 ---  @TIME[13:17:53] \n",
      "13:12:47 ---  @TIME[13:12:47] \n",
      "14:19:13 ---  @TIME[14:19:13] \n",
      "06:02:43 ---  @TIME[06:02:43] \n"
     ]
    }
   ],
   "source": [
    "# Extract date and time\n",
    "def extract_date_and_time(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_fix = re.compile('^[$Â£%â‚¬][-+][0-9]')\n",
    "    time_regex = re.compile('([0-9]{1,2}:[0-9]{1,2}:[0-9]{1,4})')\n",
    "    date_regex = re.compile('([0-9]{1,4}\\/[0-9]{1,2}\\/[0-9]{1,4})')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "\n",
    "        ## -------- Time\n",
    "        time_result = time_regex.search(prefilter)\n",
    "        if time_result:\n",
    "            prefix = prefilter[:time_result.start()]\n",
    "            suffix = prefilter[time_result.end():]\n",
    "            mpart = prefilter[time_result.start():time_result.end()]\n",
    "            temp_dict[word] = ' '.join([\n",
    "                prefix,\n",
    "                place_hold(str(mpart), TIME_TAG),\n",
    "                suffix\n",
    "            ])\n",
    "            continue\n",
    "\n",
    "        ## -------- Date\n",
    "        date_result = date_regex.search(prefilter.replace('-', '/'))\n",
    "        if date_result and len(word.split('/')) == 3:\n",
    "            prefix = prefilter[:date_result.start()]\n",
    "            suffix = prefilter[date_result.end():]\n",
    "            mpart = prefilter[date_result.start():date_result.end()]\n",
    "            temp_dict[word] = ' '.join([\n",
    "                prefix,\n",
    "                place_hold(str(mpart), DATE_TAG),\n",
    "                suffix\n",
    "            ])\n",
    "            continue\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Extract date and time:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_date_and_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 46985 | Known words: 8523\n",
      "b4 --- before\n",
      "mkt --- market\n",
      "u.s. --- united states\n",
      "chg --- change\n"
     ]
    }
   ],
   "source": [
    "def custom_global_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_general_synonyms:\n",
    "            temp_dict[word] = helper_custom_general_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom global word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_global_synonyms)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break short words:\n",
      "Unknown words: 46617 | Known words: 8546\n",
      "16th/s --- 16th / s\n",
      "6/10 --- 6 / 10\n",
      "p/e --- p / e\n",
      "44/100 --- 44 / 100\n",
      "#arpa/#btc --- #arpa / #btc\n",
      "2021/clubhouse --- 2021 / clubhouse\n",
      "$theta/ --- $theta / \n",
      "76/100 --- 76 / 100\n",
      "/r/wallstreetbets, ---  / r / wallstreetbets,\n",
      "4/ --- 4 / \n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "def break_short_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break short words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(break_short_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 46595 | Known words: 8551\n",
      "ripconnnneeeeecccctt. --- ripconnnneeeeecccctt . \n",
      "trx,doge,xrp,kava,xlm,bel,cvc --- trx , doge , xrp , kava , xlm , bel , cvc\n",
      "poker/blackjack/roulette --- poker / blackjack / roulette\n",
      "sell-bitcoin-btc-for-usd-in-united-states --- sell bitcoin btc for usd in united states\n",
      "everybody's-getting-#bitcoin-for-birthdays --- everybody's getting #bitcoin for birthdays\n",
      "noche/madrugada/manana --- noche / madrugada / manana\n",
      "buy-bitcoin-btc-for-aud-in-kenya --- buy bitcoin btc for aud in kenya\n",
      "34650-34850-35050-35250-35450 --- 34650 34850 35050 35250 35450\n",
      "better,faster,chesper --- better , faster , chesper\n",
      "range:01/01/2016-02/01/2021 --- range : 01/01/2016-02/01/2021\n",
      "########## Step - Break long words:\n",
      "Unknown words: 46594 | Known words: 8551\n",
      "reddit/robinhood/gamestop --- reddit / robinhood / gamestop\n",
      "level,thankyouverymuch --- level , thankyouverymuch\n",
      "01/01/2016-02/01/2021 --- 01 / 01 / 2016-02 / 01 / 2021\n",
      "########## Step - Break long words:\n",
      "Unknown words: 46594 | Known words: 8551\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "def break_long_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '_' in word and not (len(word) > 2 and word[0] in ['#', '$', '@'] and word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            temp_dict[word] = re.sub('_', ' ', word)\n",
    "        elif '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "        elif len(' '.join(word.split('-')).split())>2:\n",
    "            temp_dict[word] = re.sub('-', ' ', word)\n",
    "        for s in ',.:;':\n",
    "            if s in word and not re.compile('[+#@$/,.:;-]').sub('', word).isnumeric():\n",
    "                temp_dict[word] = word.replace(s, f' {s} ')\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "for i in range(3):\n",
    "    texts = texts.pipe(break_long_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 46502 | Known words: 8553\n",
      ",#xrp --- , #xrp\n",
      ".#online --- . #online\n",
      "12#in --- 12 #in\n",
      ".@elonmusk --- . @elonmusk\n",
      "goal.#nextprotocol --- goal. #nextprotocol\n",
      "us$35 --- us $35\n",
      "iui#btc --- iui #btc\n",
      "1.61-$11 --- 1.61- $11\n",
      "~$35k --- ~ $35k\n",
      "ai#btc --- ai #btc\n"
     ]
    }
   ],
   "source": [
    "# TODO: add number parsing before\n",
    "# Diambiguate entities\n",
    "# Split words on @,# and $ to clear up ambiguities between entitites\n",
    "def disambiguate_entitites(texts):\n",
    "    symbols = '@#$'\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('@' in k or '#' in k or '$' in k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        for symbol in symbols:\n",
    "            if symbol not in word: continue\n",
    "            left, *right = word.split(symbol)\n",
    "            rightz = symbol.join(right)\n",
    "            if len(left) > 0 and len(right[0]) > 0 and right[0].isalnum():\n",
    "                temp_dict[word] = f'{left} {symbol}{rightz}'\n",
    "            break\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Disambiguate entities:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(disambiguate_entitites)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 46463 | Known words: 8553\n",
      "$hodl --- #hodl\n",
      "bitstamp --- @bitstamp\n",
      "@blockchain --- #blockchain\n",
      "coinbase --- @coinbase\n",
      "#bittrex --- @bittrex\n",
      "paypal --- @paypal\n",
      "#coinbase --- @coinbase\n",
      "$binance --- @binance\n",
      "#bitmex --- @bitmex\n",
      "#altcoin --- #altcoins\n"
     ]
    }
   ],
   "source": [
    "def custom_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_synonyms:\n",
    "            temp_dict[word] = helper_custom_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 46173 | Known words: 8553\n",
      "comp --- $compound\n",
      "$cro --- $crypto_com_coin\n",
      "$cas --- $cashaa\n",
      "$nmc --- $namecoin\n",
      "yfi --- $yearn_finance\n",
      "$nav --- $nav_coin\n",
      "$audio --- $audius\n",
      "#qtum --- $qtum\n",
      "$orn --- $orion_protocol\n",
      "$cover --- $cover_protocol_new\n"
     ]
    }
   ],
   "source": [
    "def custom_currency_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_currency_synonyms:\n",
    "            temp_dict[word] = helper_currency_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom currency synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_currency_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 45459 | Known words: 8553\n",
      "@cardosogerman --- @USR[cardosogerman]\n",
      "#binanceexchange --- @HTAG[binanceexchange]\n",
      "@followarmysbts --- @USR[followarmysbts]\n",
      "#speculators --- @HTAG[speculators]\n",
      "#decentralization --- @HTAG[decentralization]\n",
      "$uvxy --- @HTAG[uvxy]\n",
      "#bigpumpsignal --- @HTAG[bigpumpsignal]\n",
      "@bloomberg --- @USR[bloomberg]\n",
      "#casino --- @HTAG[casino]\n",
      "#financialeducation --- @HTAG[financialeducation]\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags\n",
    "def extract_entities(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(word) > 2) and (word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            new_word = word.replace('\\'s', '')\n",
    "            if not re.compile('[#@$/,.:;]').sub('', new_word).isnumeric():\n",
    "                new_word = re.compile('[,.:;]').sub('', new_word)\n",
    "                if word.startswith('@'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], USER_TAG)\n",
    "                elif word.startswith('#'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], HASH_TAG)\n",
    "                elif word.startswith('u/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], USER_TAG)\n",
    "                elif word.startswith('r/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], HASH_TAG)\n",
    "                elif word.startswith('$') and new_word[1:].replace('_', '').isalpha():\n",
    "                    tag = CURRENCY_TAG if word[1:] in helper_currency_synonyms else HASH_TAG\n",
    "                    temp_dict[word] = place_hold(new_word[1:], tag)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 45432 | Known words: 8553\n",
      "@HTAG[link] --- @CURR[link]\n",
      "@HTAG[xrp] --- @CURR[xrp]\n",
      "@HTAG[zilliqa] --- @CURR[zilliqa]\n",
      "@HTAG[qtum] --- @CURR[qtum]\n",
      "@HTAG[dash] --- @CURR[dash]\n",
      "@HTAG[litecoin] --- @CURR[litecoin]\n",
      "@HTAG[tezos] --- @CURR[tezos]\n",
      "@HTAG[bitcoin] --- @CURR[bitcoin]\n",
      "@USR[bitcoin] --- @CURR[bitcoin]\n",
      "@HTAG[cardano] --- @CURR[cardano]\n"
     ]
    }
   ],
   "source": [
    "# Hashtag and currency union\n",
    "def hashtag_currency_union(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = set([k for k in temp_vocab if not check_replace(k)])\n",
    "    temp_dict = {}\n",
    "    for w in temp_vocab:\n",
    "        if w.startswith(CURRENCY_TAG):\n",
    "            if w.replace(CURRENCY_TAG, HASH_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, HASH_TAG)] = w\n",
    "            if w.replace(CURRENCY_TAG, USER_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, USER_TAG)] = w\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Hashtag and currency union:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 45431 | Known words: 8553\n",
      "zennf_ --- zennf\n",
      "_h_o_u_r_s_ --- _h_o_u_r_s\n",
      "_i_n_ --- _i_n\n",
      "._ --- .\n",
      "e_n_d_s_ --- e_n_d_s\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "def remove_ending_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[len(word)-1]=='_':\n",
    "            for i in range(len(word),0,-1):\n",
    "                if word[i-1]!='_':\n",
    "                    new_word = word[:i]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_ending_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 45431 | Known words: 8553\n",
      "_h_o_u_r_s --- h_o_u_r_s\n",
      "_i_n --- i_n\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore\n",
    "def remove_starting_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[0]=='_':\n",
    "            for i in range(len(word)):\n",
    "                if word[i]!='_':\n",
    "                    new_word = word[i:]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_starting_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 37616 | Known words: 9040\n",
      "crypto: --- crypto :\n",
      "mind-bender, --- mind-bender ,\n",
      "clue. --- clue .\n",
      "rappers, --- rappers ,\n",
      "'21, --- '21 ,\n",
      "'stake' --- 'stake '\n",
      "informations. --- informations .\n",
      "meme! --- meme !\n",
      "semana. --- semana .\n",
      "fund. --- fund .\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "def end_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1].isnumeric() and re.compile('[$Â£%â‚¬]').match(word[i]):\n",
    "                break\n",
    "\n",
    "            if word[i-1].isalnum():\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(end_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 36070 | Known words: 9052\n",
      "$1488.36 --- @NUM[1488.36] usd\n",
      "4.63% --- @NUM[4.63] percent\n",
      "0xce1f27b591ca205066ac9257e3cab7b604a457b4 --- @NUM[0.0] xce1f27b591ca205066ac9257e3cab7b604a457b4\n",
      "$28k --- @NUM[28000.0] usd\n",
      "193.50$ --- @NUM[193.5] usd\n",
      "424,017 --- @NUM[424017.0]\n",
      "171.18$ --- @NUM[171.18] usd\n",
      "1.63% --- @NUM[1.63] percent\n",
      "2k --- @NUM[2000.0]\n",
      "44,903 --- @NUM[44903.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35971 | Known words: 9052\n",
      "2k --- @NUM[2000.0]\n",
      "0.00145 --- @NUM[145.0]\n",
      "17% --- @NUM[17.0] percent\n",
      "0.45 --- @NUM[0.45]\n",
      "2022 --- @NUM[2022.0]\n",
      "+35 --- @NUM[35.0]\n",
      "0.00135 --- @NUM[135.0]\n",
      "0.0014 --- @NUM[14.0]\n",
      "10pm --- @NUM[10.0] pm\n",
      "21,621 --- @NUM[21621.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35968 | Known words: 9052\n",
      "+5x --- @NUM[5.0] times\n",
      "=72k --- equals @NUM[72000.0]\n",
      "=1.500 --- equals @NUM[1500.0]\n",
      "=11k --- equals @NUM[11000.0]\n",
      "*365 --- @NUM[365.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35968 | Known words: 9052\n"
     ]
    }
   ],
   "source": [
    "scale_mapping = {\n",
    "    'b': 1000000000,\n",
    "    'bn': 1000000000,\n",
    "    'bln': 1000000000,\n",
    "    'billion': 1000000000,\n",
    "    'm': 1000000,\n",
    "    'mn': 1000000,\n",
    "    'mln': 1000000,\n",
    "    'million': 1000000,\n",
    "    'k': 1000,\n",
    "    'thousand': 1000,\n",
    "    '-': -1,\n",
    "}\n",
    "\n",
    "translate = {\n",
    "    '$': 'usd', 'Â£': 'gbp','%': 'percent', 'â‚¬': 'eur'\n",
    "}\n",
    "\n",
    "translate_suffix = {\n",
    "    'x': 'times'\n",
    "}\n",
    "\n",
    "translate_prefix = {\n",
    "    '~': 'around',\n",
    "    '+-': 'around',\n",
    "    'Â±': 'around',\n",
    "    '@': 'at',\n",
    "    '=': 'equals',\n",
    "    '*#': 'ranked',\n",
    "    '#': 'ranked',\n",
    "}\n",
    "\n",
    "def serialize_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_num = re.compile('^(~|\\+-|Â±|@|=|#|\\*#)?[-@+*^#:]?[$Â£%â‚¬]?(([.:]?[0-9])+)[$Â£%â‚¬]?')\n",
    "    re_fix = re.compile('^[$Â£%â‚¬][-+][0-9]')\n",
    "    time_regex = re.compile('([0-9]{1,2}:[0-9]{1,2}:[0-9]{1,4})')\n",
    "    date_regex = re.compile('([0-9]{1,4}\\/[0-9]{1,2}\\/[0-9]{1,4})')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "\n",
    "        ## ----- Various other numbers\n",
    "        result = re_num.search(prefilter)\n",
    "        if result and result.pos == 0:\n",
    "            # Process combined numbers / ranges in next iteration\n",
    "            if '-' in word and not word.startswith('-') and not word.startswith('+-'):\n",
    "                temp_dict[word] = ' '.join(word.split('-'))\n",
    "                continue\n",
    "\n",
    "            main_part = prefilter[:result.end()]\n",
    "            prefix = ''\n",
    "            for prefix_key, prefix_name in translate_prefix.items():\n",
    "                if main_part.startswith(prefix_key):\n",
    "                    prefix = prefix_name\n",
    "                    main_part = main_part.replace(prefix_key, '', 1)\n",
    "                    break\n",
    "\n",
    "            main = re.compile('^[~@+*^#:]').sub('',main_part)\n",
    "            currency = re.compile('[$Â£%â‚¬]').search(main)\n",
    "            currency = main[currency.start():currency.end()] if currency else None\n",
    "            main = re.compile('[$Â£%â‚¬]').sub('', main)\n",
    "            suffix = prefilter[result.end():]\n",
    "\n",
    "            multiplier = 1\n",
    "            if re.compile('\\.[0-9]{1,2}$').search(main): # decimal\n",
    "                multiplier *= 0.01 if main[-1].isnumeric() else 0.1\n",
    "            if '-' in main: # Neg numbers\n",
    "                multiplier *= -1\n",
    "                main = main.replace('-', '')\n",
    "            # Textual scale\n",
    "            if suffix in scale_mapping:\n",
    "                multiplier *= scale_mapping[suffix]\n",
    "                suffix = ''\n",
    "            if suffix in translate_suffix:\n",
    "                suffix = translate_suffix[suffix]\n",
    "\n",
    "            number = round(float(main.replace('.', '').replace(':', '')) * multiplier, 2)\n",
    "            # print(f'{number}  /  {currency}  /  {suffix}  /  {word}')\n",
    "            # noinspection PyTypeChecker\n",
    "            temp_dict[word] = ' '.join(filter(len,[\n",
    "                prefix,\n",
    "                place_hold(str(number), NUMBER_TAG),\n",
    "                translate[currency] if currency else '',\n",
    "                suffix\n",
    "            ]))\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Serialize numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Clean up numbers\n",
    "for i in range(4):\n",
    "    texts = texts.pipe(serialize_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 35967 | Known words: 9052\n",
      "chg --- change\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 35968 | Known words: 9052\n",
      ".@elonmusk --- . @elonmusk\n",
      "#innitialcoinoffering@myidentitycoin --- #innitialcoinoffering @myidentitycoin\n",
      "'#bitcoin --- ' #bitcoin\n",
      "|#litecoin --- | #litecoin\n",
      ".@moneyonchainok --- . @moneyonchainok\n",
      "us$450 --- us $450\n",
      ".@peterschiff --- . @peterschiff\n",
      "guap@s --- guap @s\n",
      ".@joebiden --- . @joebiden\n",
      ".@jack --- . @jack\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 35964 | Known words: 9052\n",
      "coinbase --- @coinbase\n",
      "paypal --- @paypal\n",
      "cointelegraph --- @cointelegraph\n",
      "binance --- @binance\n",
      "airdrop --- #airdrop\n",
      "altcoin --- #altcoins\n",
      "blockchain --- #blockchain\n",
      "bittrex --- @bittrex\n",
      "dogecoins --- $dogecoin\n",
      "#dogecoins --- $dogecoin\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 35942 | Known words: 9052\n",
      "yfi --- $yearn_finance\n",
      "#bitcoin --- $bitcoin\n",
      "jpy --- $jpy\n",
      "$gme --- $gamestop_tokenized_stock_ftx\n",
      "uniswap --- $uniswap\n",
      "$mrph --- $morpheus_network\n",
      "$grt --- $golden_ratio_token\n",
      "aave --- $aave\n",
      "$doge --- $dogecoin\n",
      "ltc --- $litecoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 35865 | Known words: 9052\n",
      "$usd --- @CURR[usd]\n",
      "$jpy --- @CURR[jpy]\n",
      "@joebiden --- @USR[joebiden]\n",
      "$vechain --- @CURR[vechain]\n",
      "$tezos --- @CURR[tezos]\n",
      "$hard_protocol --- @HTAG[hard_protocol]\n",
      "$usd_coin --- @CURR[usd_coin]\n",
      "$eur --- @CURR[eur]\n",
      "$digibyte --- @CURR[digibyte]\n",
      "$binance_coin --- @CURR[binance_coin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 35865 | Known words: 9052\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 35558 | Known words: 9069\n",
      "'but --- ' but\n",
      "'pragmatic --- ' pragmatic\n",
      "Â¥237500.02 --- Â¥ 237500.02\n",
      "Â¿posee --- Â¿ posee\n",
      "'units --- ' units\n",
      "*valentine --- * valentine\n",
      "Â¿cuales --- Â¿ cuales\n",
      "-s --- - s\n",
      "'gone --- ' gone\n",
      "***we --- *** we\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "def start_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum() and k[0] not in ['@', '#', '$'])]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word)):\n",
    "            if word[i].isalnum() or word[i] in ['#', '@', '$']:\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(start_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 35558 | Known words: 9069\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 35558 | Known words: 9069\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35555 | Known words: 9069\n",
      "3653341.0 --- @NUM[365334.1]\n",
      "218011.21 --- @NUM[218011.21]\n",
      "224373.40 --- @NUM[224373.4]\n",
      "3,767,132 --- @NUM[3767132.0]\n",
      "3631924.0 --- @NUM[363192.4]\n",
      "217126.98 --- @NUM[217126.98]\n",
      "225519.82 --- @NUM[225519.82]\n",
      "3667909.0 --- @NUM[366790.9]\n",
      "$700,283.22 --- @NUM[700283.22] usd\n",
      "$733,005.64 --- @NUM[733005.64] usd\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 35555 | Known words: 9069\n",
      "cryptocurrency --- #cryptocurrency\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 35555 | Known words: 9069\n",
      "ethereum --- $ethereum\n",
      "bitcoin --- $bitcoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 35549 | Known words: 9069\n",
      "@_cryptocurator --- @USR[_cryptocurator]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "@flow_blockchain --- @USR[flow_blockchain]\n",
      "@iohk_charles --- @USR[iohk_charles]\n",
      "@michael_saylor --- @USR[michael_saylor]\n",
      "#cryptocurrency --- @HTAG[cryptocurrency]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 35549 | Known words: 9069\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 35549 | Known words: 9069\n",
      "a.k.a --- word_placeholder[aka]\n",
      "b.a.l --- word_placeholder[bal]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "def find_replace_acronyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "            if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "                temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "            else:\n",
    "                if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                    temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(find_replace_acronyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Contractions:\n",
      "Unknown words: 35486 | Known words: 9069\n",
      "let's --- let us\n",
      "ya'll --- you will\n",
      "that'll --- that will\n",
      "would've --- would have\n",
      "i've --- i have\n",
      "i'll --- i will\n",
      "you're --- you are\n",
      "wouldn't --- would not\n",
      "you've --- you have\n",
      "weren't --- were not\n"
     ]
    }
   ],
   "source": [
    "# Apply spellchecker for contractions\n",
    "def apply_spellchecker_contractions(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_contractions:\n",
    "            temp_dict[word] = helper_contractions[word] # place_hold(helper_contractions[word])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(apply_spellchecker_contractions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove \"s:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "portfolio's --- portfolio\n",
      "rat's --- rat\n",
      "btc's --- btc\n",
      "@elonmusk's --- @elonmusk\n",
      "hl's --- hl\n",
      "bba's --- bba\n",
      "occam's --- occam\n",
      "argentina's --- argentina\n",
      "@zackvoell's --- @zackvoell\n",
      "schokobub's --- schokobub\n"
     ]
    }
   ],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "def remove_comma_s(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_comma_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "#btc\\#usdt --- #btc / #usdt\n"
     ]
    }
   ],
   "source": [
    "def convert_backslash(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]\n",
    "    temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_backslash)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 35275 | Known words: 9084\n",
      "paypal --- @paypal\n",
      "binance --- @binance\n",
      "kraken --- @kraken\n",
      "blockchain --- #blockchain\n",
      "cryptocurrency --- #cryptocurrency\n",
      "crypto --- #cryptocurrency\n",
      "hodl --- #hodl\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 35266 | Known words: 9084\n",
      "#bitcoin --- $bitcoin\n",
      "$tsla --- $tesla_tokenized_stock_bittrex\n",
      "$gme --- $gamestop_tokenized_stock_ftx\n",
      "eth --- $ethereum\n",
      "#usdt --- $tether\n",
      "#btc --- $bitcoin\n",
      "$akro --- $akropolis\n",
      "#quant --- $quant\n",
      "#ethereum --- $ethereum\n",
      "litecoin --- $litecoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 35212 | Known words: 9084\n",
      "@tesla --- @USR[tesla]\n",
      "@kraken --- @USR[kraken]\n",
      "@knutsvanholm --- @USR[knutsvanholm]\n",
      "@razor_network --- @USR[razor_network]\n",
      "@virgilgr --- @USR[virgilgr]\n",
      "@trustologyio --- @USR[trustologyio]\n",
      "$cardano --- @CURR[cardano]\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "$tron --- @CURR[tron]\n",
      "@elonmusk --- @USR[elonmusk]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 35212 | Known words: 9084\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Dup chars (with vocab check):\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "uu --- u\n",
      "ioo --- io\n",
      "jeet --- jet\n",
      "bbs --- bs\n",
      "upp --- up\n",
      "arcc --- arc\n",
      "eest --- est\n",
      "yooooou --- you\n",
      "wonnnnnnnn --- won\n",
      "ohh --- oh\n"
     ]
    }
   ],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!). TODO check fist against vocab?\n",
    "def remove_duplicated_character(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_vocab_dup = []\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "    temp_vocab_dup = set(temp_vocab_dup)\n",
    "    temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "        if new_word in temp_vocab_dup:\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_duplicated_character)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 34936 | Known words: 9122\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate numbers:\n",
      "Unknown words: 34936 | Known words: 9122\n"
     ]
    }
   ],
   "source": [
    "def isolate_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "            if re.compile('[0-9]').sub('', word) != word:\n",
    "                temp_dict[word] = word\n",
    "\n",
    "    global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    temp_dict = {k:place_hold(k) for k in temp_dict}\n",
    "\n",
    "    #texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 34928 | Known words: 9122\n",
      ".-- --- .-\n",
      "#bitcoin--but --- #bitcoin-but\n",
      "---- --- -\n",
      "-- --- -\n",
      "hahaha--and --- hahaha-and\n",
      "--------- --- -\n",
      "--- --- -\n",
      "--------------- --- -\n",
      "them--china --- them-china\n",
      "----- --- -\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "def join_dashes(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_dashes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 34927 | Known words: 9122\n",
      "fi-na-lly --- finally\n"
     ]
    }
   ],
   "source": [
    "# Try join word (Sloooow)\n",
    "def join_word_letters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "        if (new_word in local_vocab) and (len(new_word)>3):\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_word_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 33543 | Known words: 9244\n",
      "ceo'luktan --- ceo ' luktan\n",
      "d'une --- d ' une\n",
      "a:$33,781 --- a :  $ 33 , 781\n",
      "kazanacaksÄ±nÄ±z --- kazanacaks Ä± n Ä± z\n",
      "#cex.io ---  # cex . io\n",
      "polo.eth --- polo . eth\n",
      "almayÄ± --- almay Ä± \n",
      "$m ---  $ m\n",
      "ðŸƒ ---  ðŸƒ \n",
      "navalny.eth --- navalny . eth\n"
     ]
    }
   ],
   "source": [
    "# TODO: _ should become ' ' and we should preserve numbers or hashtags\n",
    "# Try Split word\n",
    "def split_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "            chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "            temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(split_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 33542 | Known words: 9244\n",
      "algor1thms --- algorithms\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion\n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "\n",
    "def convert_leet_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = convert_leet(word)\n",
    "        if (new_word!=word):\n",
    "            if (len(word)>2) and (new_word in local_vocab):\n",
    "                temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_leet_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 33542 | Known words: 9244\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 33500 | Known words: 9245\n",
      "882 --- @NUM[882.0]\n",
      "503 --- @NUM[503.0]\n",
      "886857 --- @NUM[886857.0]\n",
      "47439 --- @NUM[47439.0]\n",
      "36923 --- @NUM[36923.0]\n",
      "33670 --- @NUM[33670.0]\n",
      "33197 --- @NUM[33197.0]\n",
      "429 --- @NUM[429.0]\n",
      "060 --- @NUM[60.0]\n",
      "21598 --- @NUM[21598.0]\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 33499 | Known words: 9245\n",
      "bitstamp --- @bitstamp\n",
      "coinbase --- @coinbase\n",
      "paypal --- @paypal\n",
      "binance --- @binance\n",
      "airdrop --- #airdrop\n",
      "blockchain --- #blockchain\n",
      "bittrex --- @bittrex\n",
      "cryptocurrency --- #cryptocurrency\n",
      "crypto --- #cryptocurrency\n",
      "bitmex --- @bitmex\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 33494 | Known words: 9245\n",
      "comp --- $compound\n",
      "ont --- $ontology\n",
      "wbtc --- $wrapped_bitcoin\n",
      "aave --- $aave\n",
      "ltc --- $litecoin\n",
      "egld --- $elrond_egld\n",
      "ils --- $ils\n",
      "bch --- $bitcoin_cash\n",
      "usdc --- $usd_coin\n",
      "elrond --- $elrond_egld\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 33456 | Known words: 9245\n",
      "$usd_coin --- @CURR[usd_coin]\n",
      "$binance_coin --- @CURR[binance_coin]\n",
      "$thorchain --- @CURR[thorchain]\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "$tron --- @CURR[tron]\n",
      "$bitcoin_cash --- @CURR[bitcoin_cash]\n",
      "$xrp --- @CURR[xrp]\n",
      "$compound --- @HTAG[compound]\n",
      "#hodl --- @HTAG[hodl]\n",
      "$hedera_hashgraph --- @CURR[hedera_hashgraph]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 33456 | Known words: 9245\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 33454 | Known words: 9245\n"
     ]
    }
   ],
   "source": [
    "# Remove placeholders\n",
    "def remove_placeholders(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (not check_replace(k) and k.startswith(WPLACEHOLDER))]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_placeholders)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "nuevas --- nueva\n",
      "winnings --- winning\n",
      "subscriptions --- subscription\n",
      "chads --- chad\n",
      "collaborates --- collaborate\n",
      "dinos --- dino\n",
      "audits --- audit\n",
      "billionaires --- billionaire\n",
      "grupos --- grupo\n",
      "goldmans --- goldman\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "def search_multiple_form(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "    temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(search_multiple_form)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 33116 | Known words: 9322\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: 0.1362\n",
      "########## Step - Language datection:\n",
      "Unknown words: 22017 | Known words: 8817\n"
     ]
    }
   ],
   "source": [
    "# Cut away non english tweets\n",
    "model = fasttext.load_model('../../data/kaggle/lid.176.ftz')\n",
    "\n",
    "def langcheck(item, min_confidence=0.2):\n",
    "    text = ' '.join([w for w in item.split() if not w.startswith('@')])\n",
    "    if len(text) < 3:\n",
    "        return True\n",
    "    results = dict(zip(*model.predict(text, k=2)))\n",
    "    return results.get('__label__en', 0) > min_confidence\n",
    "\n",
    "mask = texts.parallel_map(langcheck)\n",
    "if verbose: print(f'Deleted: {1 - sum(mask)/len(texts)}')\n",
    "texts = texts[mask]\n",
    "data = data[mask]\n",
    "if verbose: print('#' * 10, 'Step - Language datection:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "                       _id  \\\n0      1356029514439155714   \n1      1356029517123514371   \n2      1356029540590616577   \n3      1356029561264349185   \n4      1356029557757726722   \n...                    ...   \n19995  1356873779323031553   \n19996  1356873843177119744   \n19997  1356873851267874817   \n19998  1356873877733969921   \n19999  1356873897111670784   \n\n                                                                                                                                                                                                          text  \n0                                                                                                                                         edmonton oilers vs ottawa senators . @CURR[bitcoin] @HTAG[betting] -  \n1                                                                      one @CURR[bitcoin] now worth @NUM[33141677.0] usd . market cap @NUM[616963.0] usd billion . based on @HTAG[coindesk] bpi @CURR[bitcoin]  \n2      @USR[dogecoinrich] @USR[dogecoinrise] i have made @NUM[20000.0] usd with @CURR[dogecoin] so far . its not that big but i want to share my profit who doesnt have a chance to board on a train . plea...  \n3      india proposed @HTAG[cryptocurrency] ban has investors nervous , may feed anti - @CURR[bitcoin] narrative @CURR[bitcoin] @HTAG[criptomonedas] @HTAG[trading] @HTAG[volatilidad] @HTAG[pypro] @CURR[b...  \n4                                                                                                                                          what are the coin to rise up ? pld tell me now ! ! ! @CURR[bitcoin]  \n...                                                                                                                                                                                                        ...  \n19995                                                                                                                                                                       @USR[meekmill] plan @CURR[bitcoin]  \n19996                                                                                                                                    @USR[jtjeremybtc] also this guy said that @CURR[bitcoin] going to 0 ðŸ˜  \n19997  we are still in the early stages of a project with over @NUM[500.0] usd million usd market cap , most projects are pump and dump . @HTAG[golden_ratio_token] is here to stay have a @CURR[the_graph]...  \n19998  riot @HTAG[blockchain] mined 222 @CURR[bitcoin] in the last quarter . and is valued at @NUM[1350000000.0] usd argo @HTAG[blockchain] mined 305 @CURR[bitcoin] in the last quarter . well you see whe...  \n19999                                                                                     @HTAG[aurixapp] @HTAG[aurix] @HTAG[cryptocurrency] @HTAG[aurixexchange] @HTAG[aur] @HTAG[aurixtoken] @HTAG[exchange]  \n\n[17276 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1356029514439155714</td>\n      <td>edmonton oilers vs ottawa senators . @CURR[bitcoin] @HTAG[betting] -</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1356029517123514371</td>\n      <td>one @CURR[bitcoin] now worth @NUM[33141677.0] usd . market cap @NUM[616963.0] usd billion . based on @HTAG[coindesk] bpi @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1356029540590616577</td>\n      <td>@USR[dogecoinrich] @USR[dogecoinrise] i have made @NUM[20000.0] usd with @CURR[dogecoin] so far . its not that big but i want to share my profit who doesnt have a chance to board on a train . plea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1356029561264349185</td>\n      <td>india proposed @HTAG[cryptocurrency] ban has investors nervous , may feed anti - @CURR[bitcoin] narrative @CURR[bitcoin] @HTAG[criptomonedas] @HTAG[trading] @HTAG[volatilidad] @HTAG[pypro] @CURR[b...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1356029557757726722</td>\n      <td>what are the coin to rise up ? pld tell me now ! ! ! @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>1356873779323031553</td>\n      <td>@USR[meekmill] plan @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>1356873843177119744</td>\n      <td>@USR[jtjeremybtc] also this guy said that @CURR[bitcoin] going to 0 ðŸ˜</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>1356873851267874817</td>\n      <td>we are still in the early stages of a project with over @NUM[500.0] usd million usd market cap , most projects are pump and dump . @HTAG[golden_ratio_token] is here to stay have a @CURR[the_graph]...</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>1356873877733969921</td>\n      <td>riot @HTAG[blockchain] mined 222 @CURR[bitcoin] in the last quarter . and is valued at @NUM[1350000000.0] usd argo @HTAG[blockchain] mined 305 @CURR[bitcoin] in the last quarter . well you see whe...</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>1356873897111670784</td>\n      <td>@HTAG[aurixapp] @HTAG[aurix] @HTAG[cryptocurrency] @HTAG[aurixexchange] @HTAG[aur] @HTAG[aurixtoken] @HTAG[exchange]</td>\n    </tr>\n  </tbody>\n</table>\n<p>17276 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = texts\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO:\n",
    "* numbers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}