{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Credit for some parts to: https://www.kaggle.com/kyakovlev/preprocessing-bert-public\n",
    "# Number extraction and hashtags is my baby\n",
    "\n",
    "# General imports|  \n",
    "import pandas as pd\n",
    "import re, warnings, pickle, itertools, emoji, unicodedata\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from utils.datasets import *\n",
    "from pandarallel import pandarallel\n",
    "import fasttext\n",
    "\n",
    "pandarallel.initialize()\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Initial vars\n",
    "\n",
    "HELPER_PATH             = '../../data/helpers/'\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "verbose = True\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "URL_TAG = '@URL'\n",
    "USER_TAG = '@USR'\n",
    "NUMBER_TAG = '@NUM'\n",
    "HASH_TAG = '@HTAG'\n",
    "CURRENCY_TAG = '@CURR'\n",
    "IMMUTABLES = [WPLACEHOLDER, URL_TAG, USER_TAG, NUMBER_TAG, HASH_TAG, CURRENCY_TAG]\n",
    "\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Preprocess helpers\n",
    "def place_hold(w, tag=WPLACEHOLDER):\n",
    "    return tag + '[' + re.sub(' ', '___', w) + ']'\n",
    "\n",
    "## Helpers\n",
    "def check_replace(w):\n",
    "    return not bool(re.search('|'.join(IMMUTABLES), w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def make_dict_cleaning(s, w_dict, skip_check=False):\n",
    "    # Replaces a word using dict if it is mutable\n",
    "    if skip_check or check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "## Get basic helper data\n",
    "\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "helper_custom_synonyms     = load_helper_file('helper_custom_synonyms')\n",
    "helper_currency_synonyms     = load_helper_file('helper_currency_synonyms')\n",
    "emoji_dict = set(e for lang in emoji.UNICODE_EMOJI.values() for e in lang)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "## Load Data\n",
    "good_cols       = ['_id', 'text']\n",
    "data = pd.read_parquet('../../data/bitcoin_twitter_raw/part_0.parquet')\n",
    "data = data.iloc[:20000][good_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 63451 | Known words: 6880\n"
     ]
    }
   ],
   "source": [
    "## Start preprocessing\n",
    "texts = data['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "global_lower=True\n",
    "texts = texts.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 54216 | Known words: 7938\n"
     ]
    }
   ],
   "source": [
    "def lower(texts):\n",
    "    texts = texts.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "if global_lower:\n",
    "    texts = texts.pipe(lower)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 53957 | Known words: 7946\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "def normalize_chars(texts):\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "    texts = texts.apply(lambda x: deaccent(x))\n",
    "    if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 53957 | Known words: 7946\n"
     ]
    }
   ],
   "source": [
    "def remove_control_chars(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_control_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 53957 | Known words: 7946\n"
     ]
    }
   ],
   "source": [
    "def remove_hrefs(texts):\n",
    "    texts = texts.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "    if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_hrefs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 53826 | Known words: 7956\n",
      "ðŸ‡²ðŸ‡¹Ùªð‚ë„Æ€ðžðŸ²í–‰ë”å•†ðŸ‡·äº†ð áµ›ð’Žð’é™†ð–“ðŸ“ðŸê¸°ð‘»ì •ðŸ†‚ê¸¸ð–‰ë°˜ðŸ­â–´ð›ðŸ™ØŸó §à¸‚ð’…à¸¿ð’ðŸ‡¬ðŸ‡»à¹„ð•®à¸”ð«Â¯ð–‹âƒ£ðŸšð‘¾ðŸ‡§ð¡ä»·ðŸ†ƒê®†ë°”çº¦ð’”ð–˜ðŸ‡µå´å€¼ð’‰ê°€ð¥â–ˆð¨ðŸ˜â¯ì½”ë©´äº¤ðŸ…»ð–†ðŸµðŸ°ðâ‚¿â‚¦â‚³ð‘¼ë°ðŸ‡ªç‰¹íŠ¸ðŸ…³íƒ‘â€â¦âž¤ë‹ˆì§€ð–Žà¸°ã… ðŸ…·ê¶ŒðŸ‡¸ð’Šï¿¼ðŸ‡ºà¸ˆë¦¬â©ã€à¹†ðŸ¬å‹ð¦ë ‡ìµð’•åŠ¡ð–™ï¿¥ì„œðŸ…½ó ³ã…œâ€ŒðŸ‡¦ó £ð’Œð’è´§ë¹„ç¢³ì¤ì—å—ì…˜â–“à¸Šë‚´ð–—ð¯ðŸ‡¿ó ´ðŸ‡¨ð…ê®¤å¯†âœ“ì¤‘ð–•ðŸ‡©ë‹¤ð–‘ð’“ð’‚ðŸ‡°ð€ð‘²ð–”à¸„æ¡â‹°ð‘³ðŸŽâŸ¶ê®‡â“œå°ë‚˜ë¡œâ–‘ð„ë ¤å¯’æ¨¡æ¶¨ðŸ‡½íšŒë•â‚ºé“¾ìŠ¤ð–šðŸ ð­ðŸ‡­âŸ ðŸ…¼ì¸ã€‘ã†”â–ºê¹Œâ í¬è²¨â‹¯é€šð•½ëŠ”ð’„ðŸ‡´æƒ³ðŸ…´ðŸ”ð®ð’—å¸ëž¬ðŸ‡®à¹å††ð’ðŸ‡±ð¬ðŸ‡³ê·¸ï¼„ç‚®ó ¢ì‹œð–Šì•„ð–ˆð–žå¿Œìˆ˜ð’†ç•™ó ¿Ñµà¸œè·Œðš\n",
      "127474 --- m\n",
      "127481 --- t\n",
      "1642 --- \n",
      "119810 --- c\n",
      "46020 --- \n",
      "384 --- \n",
      "119838 --- e\n",
      "120818 --- \n",
      "54665 --- \n",
      "45908 --- \n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "def convert_remove_bad_symbols(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji_dict) and (c not in white_list_chars)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 53659 | Known words: 7949\n",
      "Â·Î¹ÑŒÐ±Ñˆâ€¢åŠ ÑË¢ãƒŽÙ„Ð¼â˜†â€ºà¸™Ð¿à¤¯Ø©Ñ‚Úºä»®à¤¾ãƒ«å¤§ÑØ­à¸â‰ˆã‚³â‰¥Ù¹à¤…ãƒˆØ³â‚¬ã‚¿Ø¹Ø±à¤šÐ»Ð²à¸¡ãƒ³å®‰ï¼ÑƒÙ‡ã€‚ÛŒØ§ãƒ„Ùƒà¸—à¸¢Ñ‡ã‚¤à¸žÐ¸ã‚«Ð·Ø°Ðµæ¯”Ï€Ð½Ø¬Ø´Ø¸Ù†â‚¹åŒºÐ´Ø¯Ø¡ÙˆÙÙ¾ç”ŸÚ©Ú¾Ù…à¤¬ãƒ¼Î²âˆšà¸•ÑŽâ€žã€ŠØ®Ð¾ØªÑ„à¤•Ðºâ€¦à¸­ì´Î¾Ð¶ä¸‹Øµâ—Ð³ï¼Œã€‹Ø¶Ñ€Ù‚Ú†ä¸Šãƒ’Ûà¸²à¹€â†’à¸§Ñ†Ñ‹Ð°Ø«ï¼ŸÙ€âˆžÚ¯Ñà¸¥å¹³å­¦ÙŠØ¨ãƒƒ\n",
      "183 --- \n",
      "953 --- \n",
      "1100 --- \n",
      "1073 --- \n",
      "1096 --- \n",
      "8226 --- \n",
      "21152 --- \n",
      "1089 --- \n",
      "738 --- s\n",
      "12494 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "def convert_remove_bad_symbols2(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = 'Â·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji_dict) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 53659 | Known words: 7949\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if ('<' in word) and ('>' in word):\n",
    "            for tag in html_tags:\n",
    "                if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                    temp_dict[word] = BeautifulSoup(word, 'html5lib').text\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_html_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 39204 | Known words: 7949\n",
      "https://t.co/tjltisqani --- @URL[t.co]\n",
      "https://t.co/6qi6oonrrs --- @URL[t.co]\n",
      "https://t.co/tsu2ocby6c --- @URL[t.co]\n",
      "https://t.co/6l3alhbccs --- @URL[t.co]\n",
      "https://t.co/tpjlzrbuge --- @URL[t.co]\n",
      "https://t.co/vbzpjioxm6 --- @URL[t.co]\n",
      "https://t.co/zknx4hdnhc --- @URL[t.co]\n",
      "https://t.co/d9njotk5yn --- @URL[t.co]\n",
      "https://t.co/xmxubydess --- @URL[t.co]\n",
      "https://t.co/yuhci38mp4 --- @URL[t.co]\n",
      "########## Step - Convert urls part 1.5:\n",
      "Unknown words: 39203 | Known words: 7949\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it))\n",
    "def remove_links(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "    temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "\n",
    "    for word in temp_dict:\n",
    "        new_value = temp_dict[word]\n",
    "        if word.find('http')>2:\n",
    "            temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value, URL_TAG)\n",
    "        else:\n",
    "            temp_dict[word] = place_hold(new_value, URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "\n",
    "    # Remove twitter urls\n",
    "    temp_dict = {\n",
    "        f'{URL_TAG}[t.co]': ''\n",
    "    }\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1.5:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove escaped html:\n",
      "Unknown words: 39129 | Known words: 7951\n",
      "&lt;$1m --- $1m\n",
      "p&amp;d --- p and d\n",
      "&lt;3 --- 3\n",
      "&lt;excluding --- excluding\n",
      "gainer-----&gt; --- gainer-----\n",
      "f@&amp;king --- f@ and king\n",
      "-&gt; --- -\n",
      "coming&gt; --- coming\n",
      "soon&gt; --- soon\n",
      "\"s&amp;p --- \"s and p\n"
     ]
    }
   ],
   "source": [
    "# Remove escaped html\n",
    "def remove_escaped_html(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    symbols = {\n",
    "        '&quot;': '',\n",
    "        '&amp;': ' and ',\n",
    "        '&lt;': '',\n",
    "        '&gt;': '',\n",
    "    }\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if any([rep in word for rep in symbols.keys()]):\n",
    "            new_word = word\n",
    "            for rep, to in symbols.items():\n",
    "                new_word = new_word.replace(rep, to)\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove escaped html:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_escaped_html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 39129 | Known words: 7951\n",
      "www.maverick-tech.con --- @URL[maverick-tech.con]\n",
      ".www.rapidsnetwork.io --- @URL[rapidsnetwork.io]\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "def convert_urls2(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        url_check = False\n",
    "        if 'file:' in word:\n",
    "            url_check = True\n",
    "        elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "            if 'Aww' not in word:\n",
    "                for d_zone in url_extensions:\n",
    "                    if '.' + d_zone in word:\n",
    "                        url_check = True\n",
    "                        break\n",
    "        elif ('/' in word) and ('.' in word):\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone + '/' in word:\n",
    "                    url_check = True\n",
    "                    break\n",
    "\n",
    "        if url_check:\n",
    "            temp_dict[word] =  place_hold(domain_search(word), URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_urls2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 39128 | Known words: 7951\n",
      ":))) --- ðŸ˜)\n",
      ":-) --- ðŸ˜\n",
      "â¬‡@crypto_off --- â¬‡@cryptðŸ˜®ff\n",
      ":-)! --- ðŸ˜!\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if (pict in word) and (len(pict)>2):\n",
    "                    temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "                elif pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 36781 | Known words: 7975\n",
      "ðŸš¦â˜â³ðŸ˜·ðŸŒ›ðŸŽ“ðŸŒ’ðŸ¦ðŸ˜‡ðŸˆðŸ—£âœðŸ’šðŸ¤´ðŸŸ¢ðŸ¦†ã€½ðŸ‘‘âœ¨ðŸŒ—ðŸŽ¬ðŸ–¤â„ðŸ¥‰ðŸ¥³â•â˜€ðŸ¤ŸðŸ½ðŸ¤²ðŸ¥²ðŸ’²ðŸŒ“ðŸ“¢ðŸ€ðŸ™‡â²ðŸ”ªðŸ“¦ðŸ¦ðŸ‘¥ðŸŽðŸ¾âœ”ðŸ®ðŸ”ŸðŸ¥œðŸ¥‚ðŸ› ðŸ¦ðŸ’­ðŸ’¥ðŸŽ†â¬ðŸŒžðŸŒðŸ˜¢ðŸ”œðŸ˜‘ðŸ’·âš ðŸ”¸ðŸ¥ƒâž¡ðŸ¦–ðŸŒ‹ðŸ¤”ðŸ¦½ðŸ”¥ðŸš©â›“ðŸ™ˆðŸŽˆðŸ™â«ðŸŽ¥ðŸ“°â™‚ðŸ’¯ðŸ˜‹â›µðŸ†ðŸš‘ðŸ™ŒðŸ™„ðŸ‘­ðŸ²ðŸŽ¤ðŸš˜ðŸ¥°ðŸ“ˆðŸ‘•ðŸ£ðŸ†’ðŸ¥¶ðŸŸ¥ðŸ–¼ðŸ¦ðŸ˜ðŸ‘ðŸ˜¬ðŸ˜±â˜•â¤´â–«ðŸ’ªðŸ¥¬ðŸš¨ðŸ’–ðŸðŸ§ðŸ¤©ðŸ¤âš½ðŸ›€ðŸ˜ ðŸ¼ðŸ¤§ðŸ–•ðŸ¥ˆðŸ·ðŸ’¡ðŸ’˜ðŸŒŠðŸ˜¤â¤ðŸ‘½ðŸ’©âœ…ðŸ˜€ðŸ‘„ðŸ˜œðŸ¥¥ðŸŒªðŸš€ðŸ”ºðŸŽ©ðŸ™ðŸ›’ðŸ˜ŒðŸ¥ºðŸ¥¸ðŸ•µðŸ¡ðŸ›¤ðŸ”ŠðŸ³ðŸ§˜ðŸŒ™â“ðŸŒ‡ðŸ¦Žâ›ªðŸ’¼â¬†ðŸ’‰ðŸ¤¢ðŸ¤™ðŸðŸŽŠðŸ’ðŸ¦¬ðŸ’¤ðŸ¯âœ³ðŸ¤¤ðŸ«ðŸ›¡â—½ðŸ™†ðŸ’«ðŸ’ â™¦ðŸ¤­ðŸ’“â¤µðŸ¤žðŸ¦ðŸ”‚ðŸ¦Šâ†©ðŸŒ¹â„¹ðŸ—‘ðŸ”ðŸ˜‚ðŸ“²ðŸ¤ðŸ•¯ðŸ’•ðŸ‚ðŸ¥±â­ðŸ˜®ðŸŒ‘âœŒðŸ”‘â±ðŸ¤šðŸ˜»ðŸ˜¥ðŸ˜’ðŸ“ðŸ’£ðŸ›°ðŸ¦ºðŸ§¿ðŸ˜µðŸðŸ”µðŸ’ƒðŸ‘¾ðŸŸ â˜®ðŸ¤¡ðŸ™ðŸ’¨ðŸ¥µðŸ¤«ðŸ“ŒðŸ’—ðŸ¸ðŸŒšðŸ”›â€¼ðŸŒŸâ‡ðŸŽ‰ðŸ”½ðŸ“žðŸŒ˜ðŸ‡ðŸ‘»ðŸ¥’ðŸ™ƒðŸŒðŸŒ ðŸ‹ðŸŒ¿ðŸºðŸ˜¡ðŸ”†ðŸš£ðŸ˜™â–¶ðŸ¤ðŸ…âš›ðŸ“±ðŸ¤ŒðŸ–â”ðŸ”¨ðŸ§¸ðŸ‘¬ðŸ„ðŸ‘ŠðŸ•ðŸ€â˜‘â™£ðŸ¥“ðŸ¤œðŸ’™ðŸ’€ðŸ”„ðŸ†™âšªðŸ”»ðŸ¦ˆðŸ˜´ðŸ˜¶ðŸ¤ªðŸ’§âœˆðŸ©¸ðŸ—¨ðŸŒðŸ”‹â˜¢ðŸ“‰ðŸŽðŸ™ŠðŸ§µðŸ‹ðŸ˜”ðŸ¼ðŸ’”ðŸš‚ðŸ—“â¬›ðŸ•ºðŸ˜›ðŸ’¶ðŸ’›ðŸ˜³ðŸºðŸŽ­ðŸ¦¡ðŸ‘‰â™€ðŸ”¼ðŸ–‡ã€°ðŸ«â¬…ðŸ”ðŸŽ£ðŸƒðŸ¬ðŸ˜ŸðŸ”±ðŸ‘¤ðŸ¥‘ðŸª…ðŸ•·ðŸ”ðŸŒ²â™‰ðŸ˜²ðŸ¥žðŸ”–ðŸš„ðŸŽ²ðŸ‘·ðŸŒ¼ðŸ—»ðŸŽðŸ§¯ðŸŒ»ðŸ¦šðŸŒˆðŸ¥•ðŸš†Â©ðŸ˜ˆðŸŒ³ðŸ¤–ðŸ”´ðŸ¦‘ðŸ¦…ðŸ¤¨ðŸŽ±ðŸ‘ˆðŸŠðŸ¤—ðŸ˜ðŸ¤›ðŸ°ðŸžðŸ˜¨ðŸ§¨âš”ðŸ¶â°ðŸ’‹ðŸ’´ðŸ˜ŠðŸ’ðŸ’ðŸ“â›³ðŸ“ŠðŸµðŸ‘Ÿâ—ðŸ¸ðŸŒ¸ðŸŽ¦ðŸ­â›´ðŸ’žðŸ—½ðŸ˜°ðŸ§¢ðŸ˜‰ðŸ„ðŸ“…ðŸ“—â˜„ðŸ’Žâ›”ðŸ’³ðŸ¤·ðŸš—ðŸ˜–ðŸ—ðŸ§™ðŸ¹â„¢ðŸ¥…ðŸƒðŸŒ§ðŸµâ›·ðŸ‘‹ðŸ•â˜ðŸ˜“ðŸ¡â™¾ðŸ§ðŸ“¹ðŸ“¡ðŸ˜§ðŸ“£ðŸ‘ºâ˜ ðŸ˜†ðŸ¦¾ðŸ”¶ðŸš«ðŸ‘‚ðŸŒðŸ‘¨ðŸŽ¯â™ŽðŸ¤¦ðŸ©ðŸ™‹ðŸ«‚ðŸ¥‡ðŸ‘¹ðŸ›«ðŸ”®ðŸŒ±ðŸ˜ŽðŸ‘©ðŸ“©ðŸ’ŠðŸ…±â¯âš«ðŸ•¶ðŸƒâœ‹ðŸ¿â›½ðŸ’¬ðŸ»ðŸ®ðŸ˜â˜Žâž•ðŸ¢ðŸ¦•ðŸ“¸ðŸ¤£ðŸ¤ðŸ’œðŸ”ƒðŸ’‡ðŸ¦—ðŸ¤“ðŸššðŸŽ°âœŠðŸ›ðŸ»ðŸ¥€ðŸ’¸ðŸŽ®ðŸ¦‹ðŸ’»ðŸŸ©ðŸ´ðŸ‘‡â™¥ðŸ¿ðŸðŸðŸŒ€âŒâ™»ðŸ§ªðŸ‘ðŸ¾ðŸš¶ðŸ˜«ðŸ¦®ðŸŽ¶ðŸ”˜ðŸ“ºðŸ¹ðŸ…°ðŸš’ðŸ˜¼ðŸ’±ðŸ‘¸ðŸ”—ðŸ˜¯ðŸš‹ðŸ¤¬ðŸ˜©ðŸªðŸŽ¨ðŸ¥©â›ðŸ§‘ðŸ³ðŸ–ðŸ”’â‰ðŸ¥›ðŸŒœðŸ†—ðŸ”«ðŸ˜ðŸ§¡ðŸ‘â˜¹ðŸ”ŽðŸ¤¸ðŸ ðŸ‘ðŸŸ§ðŸ˜ƒðŸ‘âš™ðŸ––ðŸšŠðŸ˜ªðŸ¦µðŸ‘Œâš’ðŸ’°ðŸ“–ðŸ‚â¬œâ›ˆðŸª™ðŸ”€ðŸ¥ðŸŸ¨ðŸ¥®ðŸ»ðŸŽ¢ðŸ§šðŸ”¯ðŸŽ§ðŸ¦„âš¡ðŸª–â›…ðŸ‘¶ðŸ¦§âŒšðŸ“â–ªðŸ¤ ðŸ§·â†ªðŸ¼ðŸ’µðŸ”·âšœðŸŸâž–ðŸ›¸ðŸ’¦ðŸ˜„ðŸ˜…ðŸŽ„ðŸ–ŒðŸŽ‡ðŸ¥´ðŸ“†ðŸ’ŸðŸ¦‰ðŸ›‘ðŸ‘€ðŸ™€ðŸˆðŸ§„ðŸ¤‘ðŸŽŸðŸ¤˜ðŸ˜­ðŸŽžðŸª¦ðŸŽ–ðŸ“¯Â®ðŸŽµðŸŒƒðŸ‘ŽðŸ˜ðŸ¦žðŸ“ðŸŒŽðŸŒ•ðŸ””ðŸŽðŸŒ´ðŸ“šâŒ›ðŸ”ŒðŸŒ”â†—â£ðŸ¤¯ðŸ˜šðŸŒ–ðŸ•ŠðŸ‘£ðŸ·ðŸ”¹ðŸ”ðŸ™…ðŸŒðŸ™‚ðŸ˜˜ðŸ§§ðŸ„ðŸ›ŽðŸ†šðŸ•˜ðŸ¾ðŸ¤³ðŸŒŒâ¬‡ðŸ¥ŠðŸ’¹ðŸ‘†ðŸ§ ðŸ—³ðŸ˜žðŸ¦¢ðŸ’Œâ˜ºðŸ§\n"
     ]
    }
   ],
   "source": [
    "def isolate_emoji(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if c in emoji_dict])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 34752 | Known words: 8029\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "def deduplicate_dots(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "            if (Counter(word)['.']>1):\n",
    "                new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "            if (Counter(word)['!']>1):\n",
    "                new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "            if (Counter(word)['?']>1):\n",
    "                new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "            if (Counter(word)[',']>1):\n",
    "                new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(deduplicate_dots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 34738 | Known words: 8029\n",
      "#___ --- #\n",
      "^_^ --- ^^\n",
      "#____ --- #\n",
      "_____? --- ?\n",
      "#a__ --- #a\n",
      "______ --- \n",
      "_____________________ --- \n",
      "#_ --- #\n",
      "\\_()_/ --- \\()/\n",
      "_____________ --- \n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "def remove_underscore_spam(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "            temp_dict[word] = re.sub('_', '', word)\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_underscore_spam)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 34729 | Known words: 8029\n",
      "**** ---  * \n",
      "$$$$ ---  $ \n",
      "::::::::::::::::::::::::::: ---  : \n",
      "$$$ ---  $ \n",
      "$$$$$ ---  $ \n",
      "$$$$$$$$$$$$ ---  $ \n",
      "***** ---  * \n",
      "*** ---  * \n",
      ")))) ---  ) \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "def isolate_spam_characters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "            temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(1)])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_spam_characters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 34724 | Known words: 8029\n",
      ":) --- ðŸ˜\n",
      "=) --- ðŸ˜\n",
      ":] --- ðŸ˜\n",
      ":( --- ðŸ˜¡\n",
      ";) --- ðŸ˜œ\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 33135 | Known words: 8088\n",
      "40 ---  ( \n",
      "41 ---  ) \n",
      "91 ---  [ \n",
      "93 ---  ] \n",
      "123 ---  { \n",
      "125 ---  } \n",
      "60 ---  < \n",
      "62 ---  > \n",
      "34 ---  \" \n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "def isolate_brackets(texts):\n",
    "    chars = '()[]{}<>\"'\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_brackets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 32745 | Known words: 8106\n",
      "2/6/2021 --- 2 / 6 / 2021\n",
      "$24.48/tx --- $24.48 / tx\n",
      "2021/02/08 --- 2021 / 02 / 08\n",
      "green/buy --- green / buy\n",
      "/blue: ---  / blue:\n",
      "07/02/2021 --- 07 / 02 / 2021\n",
      "50/50 --- 50 / 50\n",
      "week/month/year? --- week / month / year?\n",
      "days/weeks. --- days / weeks.\n",
      "$tfuel/ --- $tfuel / \n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "def break_short_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(break_short_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 32747 | Known words: 8110\n",
      "casino-partner/stakeholder. --- casino-partner/stakeholder . \n",
      "#the_bull_run_has_just_started. --- #the_bull_run_has_just_started . \n",
      "espadora@protonmail.com --- espadora@protonmail . com\n",
      "hurdle-turned-support --- hurdle turned support\n",
      "every-once-in-a-while, --- every-once-in-a-while , \n",
      "like/retweet/comment: --- like/retweet/comment : \n",
      "cryptosmartnow@gmail.com --- cryptosmartnow@gmail . com\n",
      "software/application. --- software/application . \n",
      "instagram@abiolaa.apparel --- instagram@abiolaa . apparel\n",
      "partnetships/integrations --- partnetships / integrations\n",
      "########## Step - Break long words:\n",
      "Unknown words: 32745 | Known words: 8110\n",
      "every-once-in-a-while --- every once in a while\n",
      "august/september/october --- august / september / october\n",
      "pullback/consolidation --- pullback / consolidation\n",
      "casino-partner/stakeholder --- casino-partner / stakeholder\n",
      "#the_bull_run_has_just_started --- #the bull run has just started\n",
      "########## Step - Break long words:\n",
      "Unknown words: 32745 | Known words: 8110\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "def break_long_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '_' in word:\n",
    "            temp_dict[word] = re.sub('_', ' ', word)\n",
    "        elif '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "        elif len(' '.join(word.split('-')).split())>2:\n",
    "            temp_dict[word] = re.sub('-', ' ', word)\n",
    "        for s in ',.:;':\n",
    "            if s in word and not re.compile('[+#@$/,.:;-]').sub('', word).isnumeric():\n",
    "                temp_dict[word] = word.replace(s, f' {s} ')\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "for i in range(3):\n",
    "    texts = texts.pipe(break_long_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 32635 | Known words: 8111\n",
      ",#chainlink --- , #chainlink\n",
      "+$30 --- + $30\n",
      "1.@tesla --- 1. @tesla\n",
      "!#bitcoin --- ! #bitcoin\n",
      "assets.$stbu --- assets. $stbu\n",
      "#btc?@elonmusk --- #btc? @elonmusk\n",
      "pro-#bitcoin --- pro- #bitcoin\n",
      "$80,000.#bitcoin --- $80,000. #bitcoin\n",
      ",#bitcoiners --- , #bitcoiners\n",
      "join.#megatron --- join. #megatron\n"
     ]
    }
   ],
   "source": [
    "# TODO: add number parsing before\n",
    "# Diambiguate entities\n",
    "# Split words on @,# and $ to clear up ambiguities between entitites\n",
    "def disambiguate_entitites(texts):\n",
    "    symbols = '@#$'\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('@' in k or '#' in k or '$' in k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        for symbol in symbols:\n",
    "            if symbol not in word: continue\n",
    "            left, *right = word.split(symbol)\n",
    "            rightz = symbol.join(right)\n",
    "            if len(left) > 0 and len(right[0]) > 0 and right[0].isalnum():\n",
    "                temp_dict[word] = f'{left} {symbol}{rightz}'\n",
    "            break\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Disambiguate entities:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(disambiguate_entitites)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 32601 | Known words: 8111\n",
      "#cointelegraph --- @cointelegraph\n",
      "@crypto --- #cryptocurrency\n",
      "#bitmain --- @bitmain\n",
      "#poloniex --- @poloniex\n",
      "poloniex --- @poloniex\n",
      "@blockchain --- #blockchain\n",
      "crypto --- #cryptocurrency\n",
      "bitmex --- @bitmex\n",
      "#dogecoins --- $dogecoin\n",
      "bitstamp --- @bitstamp\n"
     ]
    }
   ],
   "source": [
    "def custom_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_synonyms:\n",
    "            temp_dict[word] = helper_custom_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 32407 | Known words: 8111\n",
      "@uniswap --- $uniswap\n",
      "#solana --- $solana\n",
      "#polkadot --- $polkadot_new\n",
      "#bnb --- $binance_coin\n",
      "$nyan --- $nyan_finance\n",
      "#uniswap --- $uniswap\n",
      "$gum --- $gourmet_galaxy\n",
      "$pfi --- $primefinance\n",
      "$usdc --- $usd_coin\n",
      "$ndn --- $ndn_link\n"
     ]
    }
   ],
   "source": [
    "def custom_currency_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_currency_synonyms:\n",
    "            temp_dict[word] = helper_currency_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom currency synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_currency_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 31812 | Known words: 8111\n",
      "#injectiveprotocol --- @HTAG[injectiveprotocol]\n",
      "@balancerlabs --- @USR[balancerlabs]\n",
      "#getrekt --- @HTAG[getrekt]\n",
      "#datehouston --- @HTAG[datehouston]\n",
      "@crypto5s --- @USR[crypto5s]\n",
      "@ukbitcoinblog --- @USR[ukbitcoinblog]\n",
      "#whitepaper --- @HTAG[whitepaper]\n",
      "#donandjonspicks --- @HTAG[donandjonspicks]\n",
      "#stayincrypto --- @HTAG[stayincrypto]\n",
      "@nft_io --- @USR[nft_io]\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags\n",
    "def extract_entities(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(word) > 2) and (word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            new_word = word.replace('\\'s', '')\n",
    "            if not re.compile('[#@$/,.:;]').sub('', new_word).isnumeric():\n",
    "                new_word = re.compile('[,.:;]').sub('', new_word)\n",
    "                if word.startswith('@'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], USER_TAG)\n",
    "                elif word.startswith('#'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], HASH_TAG)\n",
    "                elif word.startswith('u/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], USER_TAG)\n",
    "                elif word.startswith('r/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], HASH_TAG)\n",
    "                elif word.startswith('$') and word[1:].isalpha():\n",
    "                    tag = CURRENCY_TAG if word[1:] in helper_currency_synonyms else HASH_TAG\n",
    "                    temp_dict[word] = place_hold(new_word[1:], tag)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 31789 | Known words: 8111\n",
      "@HTAG[aave] --- @CURR[aave]\n",
      "@HTAG[iota] --- @CURR[iota]\n",
      "@HTAG[celsius] --- @CURR[celsius]\n",
      "@HTAG[xrp] --- @CURR[xrp]\n",
      "@USR[algorand] --- @CURR[algorand]\n",
      "@HTAG[bitcoin] --- @CURR[bitcoin]\n",
      "@USR[bitcoin] --- @CURR[bitcoin]\n",
      "@HTAG[zilliqa] --- @CURR[zilliqa]\n",
      "@HTAG[dogecoin] --- @CURR[dogecoin]\n",
      "@USR[dogecoin] --- @CURR[dogecoin]\n"
     ]
    }
   ],
   "source": [
    "# Hashtag and currency union\n",
    "def hashtag_currency_union(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = set([k for k in temp_vocab if not check_replace(k)])\n",
    "    temp_dict = {}\n",
    "    for w in temp_vocab:\n",
    "        if w.startswith(CURRENCY_TAG):\n",
    "            if w.replace(CURRENCY_TAG, HASH_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, HASH_TAG)] = w\n",
    "            if w.replace(CURRENCY_TAG, USER_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, USER_TAG)] = w\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Hashtag and currency union:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 31789 | Known words: 8111\n",
      "'fu__ --- 'fu\n",
      "usdt_ --- usdt\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "def remove_ending_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[len(word)-1]=='_':\n",
    "            for i in range(len(word),0,-1):\n",
    "                if word[i-1]!='_':\n",
    "                    new_word = word[:i]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_ending_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 31789 | Known words: 8111\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore\n",
    "def remove_starting_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[0]=='_':\n",
    "            for i in range(len(word)):\n",
    "                if word[i]!='_':\n",
    "                    new_word = word[i:]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_starting_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 23691 | Known words: 8586\n",
      "mooned. --- mooned .\n",
      "soab! --- soab !\n",
      "missing. --- missing .\n",
      "years? --- years ?\n",
      "shown, --- shown ,\n",
      "rights. --- rights .\n",
      "tokens? --- tokens ?\n",
      "depreciation, --- depreciation ,\n",
      "left, --- left ,\n",
      "as. --- as .\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "def end_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1].isnumeric() and re.compile('[$Â£%â‚¬]').match(word[i]):\n",
    "                break\n",
    "\n",
    "            if word[i-1].isalnum():\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(end_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21873 | Known words: 8606\n",
      "6:05pm --- @NUM[605.0] pm\n",
      "24k --- @NUM[24000.0]\n",
      "$47.7 --- @NUM[4.77] dollar\n",
      "10,000ttt --- @NUM[10000.0] ttt\n",
      "0.444 --- @NUM[444.0]\n",
      "0.75% --- @NUM[0.75] percent\n",
      "+2000% --- @NUM[2000.0] percent\n",
      "$9 --- @NUM[9.0] dollar\n",
      "138.30% --- @NUM[138.3] percent\n",
      "20.43% --- @NUM[20.43] percent\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21682 | Known words: 8606\n",
      "100$300$5001000$2000 --- @NUM[100.0] dollar 300$5001000$2000\n",
      "9% --- @NUM[9.0] percent\n",
      ".93000 --- @NUM[93000.0]\n",
      "8%. --- @NUM[8.0] percent .\n",
      "250$. --- @NUM[250.0] dollar .\n",
      "16,500 --- @NUM[16500.0]\n",
      "26.1 --- @NUM[2.61]\n",
      "38750 --- @NUM[38750.0]\n",
      "35k --- @NUM[35000.0]\n",
      "25% --- @NUM[25.0] percent\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21680 | Known words: 8606\n",
      "78$ --- @NUM[78.0] dollar\n",
      "^24 --- @NUM[24.0]\n",
      "300$5001000$2000 --- @NUM[300.0] dollar 5001000$2000\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21680 | Known words: 8606\n",
      "5001000$2000 --- @NUM[5001000.0] dollar 2000\n"
     ]
    }
   ],
   "source": [
    "scale_mapping = {\n",
    "    'b': 1000000000,\n",
    "    'bn': 1000000000,\n",
    "    'bln': 1000000000,\n",
    "    'billion': 1000000000,\n",
    "    'm': 1000000,\n",
    "    'mn': 1000000,\n",
    "    'mln': 1000000,\n",
    "    'million': 1000000,\n",
    "    'k': 1000,\n",
    "    'thousand': 1000,\n",
    "    '-': -1,\n",
    "}\n",
    "\n",
    "translate = {\n",
    "    '$': 'dollar', 'Â£': 'pound','%': 'percent', 'â‚¬': 'euro'\n",
    "}\n",
    "\n",
    "translate_suffix = {\n",
    "    'x': 'times'\n",
    "}\n",
    "\n",
    "translate_prefix = {\n",
    "    '~': 'around',\n",
    "    '+-': 'around',\n",
    "    'Â±': 'around',\n",
    "    '@': 'at',\n",
    "    '=': 'equals',\n",
    "    '*#': 'ranked',\n",
    "    '#': 'ranked',\n",
    "}\n",
    "\n",
    "def serialize_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_num = re.compile('^(~|\\+-|Â±|@|=|#|\\*#)?[-@+*^#:]?[$Â£%â‚¬]?(([.:]?[0-9])+)[$Â£%â‚¬]?')\n",
    "    re_fix = re.compile('^[$Â£%â‚¬][-+][0-9]')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "        result = re_num.search(prefilter)\n",
    "\n",
    "        if result and result.pos == 0:\n",
    "            # Process combined numbers / ranges in next iteration\n",
    "            if '-' in word and not word.startswith('-') and not word.startswith('+-'):\n",
    "                temp_dict[word] = ' '.join(word.split('-'))\n",
    "                continue\n",
    "\n",
    "            main_part = prefilter[:result.end()]\n",
    "            prefix = ''\n",
    "            for prefix_key, prefix_name in translate_prefix.items():\n",
    "                if main_part.startswith(prefix_key):\n",
    "                    prefix = prefix_name\n",
    "                    main_part = main_part.replace(prefix_key, '', 1)\n",
    "                    break\n",
    "\n",
    "            main = re.compile('^[~@+*^#:]').sub('',main_part)\n",
    "            currency = re.compile('[$Â£%â‚¬]').search(main)\n",
    "            currency = main[currency.start():currency.end()] if currency else None\n",
    "            main = re.compile('[$Â£%â‚¬]').sub('', main)\n",
    "            suffix = prefilter[result.end():]\n",
    "\n",
    "            multiplier = 1\n",
    "            if re.compile('\\.[0-9]{1,2}$').search(main): # decimal\n",
    "                multiplier *= 0.01 if main[-1].isnumeric() else 0.1\n",
    "            if '-' in main: # Neg numbers\n",
    "                multiplier *= -1\n",
    "                main = main.replace('-', '')\n",
    "            # Textual scale\n",
    "            if suffix in scale_mapping:\n",
    "                multiplier *= scale_mapping[suffix]\n",
    "                suffix = ''\n",
    "            if suffix in translate_suffix:\n",
    "                suffix = translate_suffix[suffix]\n",
    "\n",
    "            number = round(float(main.replace('.', '').replace(':', '')) * multiplier, 2)\n",
    "            # print(f'{number}  /  {currency}  /  {suffix}  /  {word}')\n",
    "            # noinspection PyTypeChecker\n",
    "            temp_dict[word] = ' '.join(filter(len,[\n",
    "                prefix,\n",
    "                place_hold(str(number), NUMBER_TAG),\n",
    "                translate[currency] if currency else '',\n",
    "                suffix\n",
    "            ]))\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Serialize numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Clean up numbers\n",
    "for i in range(4):\n",
    "    texts = texts.pipe(serialize_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 21676 | Known words: 8606\n",
      "poloniex --- @poloniex\n",
      "crypto --- #cryptocurrency\n",
      "bitpay --- @bitpay\n",
      "cryptocurrencies --- #cryptocurrency\n",
      "hodl --- #hodl\n",
      "kraken --- @kraken\n",
      "altcoins --- #altcoins\n",
      "bitmain --- @bitmain\n",
      "bitcoins --- $bitcoin\n",
      "coinbase --- @coinbase\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 21595 | Known words: 8606\n",
      "$usdc --- $usd_coin\n",
      "$wrx --- $wazirx\n",
      "$trac --- $origintrail\n",
      "$cos --- $contentos\n",
      "cardano --- $cardano\n",
      "$xem --- $nem\n",
      "dgb --- $digibyte\n",
      "$fil --- $filecoin\n",
      "$ltc --- $litecoin\n",
      "$pols --- $polkastarter\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 21457 | Known words: 8606\n",
      "@poloniex --- @USR[poloniex]\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "@bitmain --- @USR[bitmain]\n",
      "$nem --- @CURR[nem]\n",
      "$swirge --- @HTAG[swirge]\n",
      "$atos --- @HTAG[atos]\n",
      "$cny --- @CURR[cny]\n",
      "$xrp --- @CURR[xrp]\n",
      "$waltonchain --- @HTAG[waltonchain]\n",
      "$contentos --- @HTAG[contentos]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 21457 | Known words: 8606\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again\n",
    "texts = texts\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 21457 | Known words: 8606\n",
      "'buy --- ' buy\n",
      "-reach --- - reach\n",
      ".sign --- . sign\n",
      "'joke --- ' joke\n",
      "'dogecoin --- ' dogecoin\n",
      "**guess --- ** guess\n",
      "-or --- - or\n",
      "'aggressive --- ' aggressive\n",
      "'team --- ' team\n",
      "-sec.gov --- - sec.gov\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "def start_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum() and k[0] not in ['@', '#', '$'])]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word)):\n",
    "            if word[i].isalnum() or word[i] in ['#', '@', '$']:\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    # texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(start_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21456 | Known words: 8606\n",
      "$0x --- @NUM[0.0] dollar times\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 21456 | Known words: 8606\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 21456 | Known words: 8606\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 21456 | Known words: 8606\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 21456 | Known words: 8606\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 21456 | Known words: 8606\n",
      "g.o.a.t --- word_placeholder[goat]\n",
      "f.i.a.t --- word_placeholder[fiat]\n",
      "p.o.d --- word_placeholder[pod]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "def find_replace_acronyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "            if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "                temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "            else:\n",
    "                if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                    temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(find_replace_acronyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Contractions:\n",
      "Unknown words: 21394 | Known words: 8606\n",
      "she's --- she is\n",
      "where's --- where is\n",
      "he's --- he is\n",
      "ya'll --- you will\n",
      "who's --- who is\n",
      "they'd --- they would\n",
      "don't --- do not\n",
      "how's --- how is\n",
      "he'll --- he will\n",
      "where'd --- where did\n"
     ]
    }
   ],
   "source": [
    "# Apply spellchecker for contractions\n",
    "def apply_spellchecker_contractions(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_contractions:\n",
    "            temp_dict[word] = helper_contractions[word] # place_hold(helper_contractions[word])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(apply_spellchecker_contractions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove \"s:\n",
      "Unknown words: 21181 | Known words: 8617\n",
      "ftx's --- ftx\n",
      "inc.'s --- inc.\n",
      "hodler's --- hodler\n",
      "germany's --- germany\n",
      "#cme's --- #cme\n",
      "#gpu's --- #gpu\n",
      "brent's --- brent\n",
      "management's --- management\n",
      "union's --- union\n",
      "$link's --- $link\n"
     ]
    }
   ],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "def remove_comma_s(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_comma_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 21181 | Known words: 8617\n",
      "\\4238285.0 ---  / 4238285.0\n",
      "\\4301056.0 ---  / 4301056.0\n",
      "\\4299147.0 ---  / 4299147.0\n",
      "\\5058389.0 ---  / 5058389.0\n",
      "\\4241491.0 ---  / 4241491.0\n",
      "\\4240291.0 ---  / 4240291.0\n",
      "\\4233436.0 ---  / 4233436.0\n"
     ]
    }
   ],
   "source": [
    "def convert_backslash(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]\n",
    "    temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_backslash)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21181 | Known words: 8617\n",
      "4301056.0 --- @NUM[430105.6]\n",
      "4299147.0 --- @NUM[429914.7]\n",
      "4238285.0 --- @NUM[423828.5]\n",
      "4241491.0 --- @NUM[424149.1]\n",
      "4233436.0 --- @NUM[423343.6]\n",
      "4240291.0 --- @NUM[424029.1]\n",
      "5058389.0 --- @NUM[505838.9]\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 21176 | Known words: 8617\n",
      "crypto --- #cryptocurrency\n",
      "#crypto --- #cryptocurrency\n",
      "coinbase --- @coinbase\n",
      "paypal --- @paypal\n",
      "blockchain --- #blockchain\n",
      "binance --- @binance\n",
      "cryptocurrency --- #cryptocurrency\n",
      "#binance --- @binance\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 21163 | Known words: 8617\n",
      "cardano --- $cardano\n",
      "elrond --- $elrond_egld\n",
      "iota --- $iota\n",
      "bitcoin --- $bitcoin\n",
      "@dogecoin --- $dogecoin\n",
      "@cardano --- $cardano\n",
      "doge --- $dogecoin\n",
      "#cardano --- $cardano\n",
      "#btc --- $bitcoin\n",
      "eth --- $ethereum\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 21116 | Known words: 8617\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "@bitfinex --- @USR[bitfinex]\n",
      "#nyzo --- @HTAG[nyzo]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "@dragonchaingang --- @USR[dragonchaingang]\n",
      "#alt --- @HTAG[alt]\n",
      "#hex --- @HTAG[hex]\n",
      "@coinmarketcap --- @USR[coinmarketcap]\n",
      "@binance --- @USR[binance]\n",
      "#fed --- @HTAG[fed]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 21116 | Known words: 8617\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Dup chars (with vocab check):\n",
      "Unknown words: 20866 | Known words: 8652\n",
      "scalling --- scaling\n",
      "annnnnnnnnd --- and\n",
      "wenn --- wen\n",
      "brr --- br\n",
      "peep --- pep\n",
      "caal --- cal\n",
      "aai --- ai\n",
      "mmmm --- m\n",
      "goooood --- god\n",
      "yeet --- yet\n"
     ]
    }
   ],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!). TODO check fist against vocab?\n",
    "def remove_duplicated_character(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_vocab_dup = []\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "    temp_vocab_dup = set(temp_vocab_dup)\n",
    "    temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "        if new_word in temp_vocab_dup:\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_duplicated_character)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 20866 | Known words: 8652\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 20866 | Known words: 8652\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 20866 | Known words: 8652\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 20866 | Known words: 8652\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 20866 | Known words: 8652\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate numbers:\n",
      "Unknown words: 20866 | Known words: 8652\n",
      "*_100% --- word_placeholder[*_100%]\n",
      ":-6.11 --- word_placeholder[:-6.11]\n"
     ]
    }
   ],
   "source": [
    "def isolate_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "            if re.compile('[0-9]').sub('', word) != word:\n",
    "                temp_dict[word] = word\n",
    "\n",
    "    global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    temp_dict = {k:place_hold(k) for k in temp_dict}\n",
    "\n",
    "    #texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 20860 | Known words: 8652\n",
      "---- --- -\n",
      "aa--tag --- aa-tag\n",
      "outshined--cryptocurrency --- outshined-cryptocurrency\n",
      "------------- --- -\n",
      "--designed --- -designed\n",
      "----- --- -\n",
      "--- --- -\n",
      "#crypto!--where --- #crypto!-where\n",
      "------------------------------------------ --- -\n",
      "clockwork--up --- clockwork-up\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "def join_dashes(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_dashes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 20860 | Known words: 8652\n"
     ]
    }
   ],
   "source": [
    "# Try join word (Sloooow)\n",
    "def join_word_letters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "        if (new_word in local_vocab) and (len(new_word)>3):\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_word_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 19538 | Known words: 8791\n",
      "ðŸŒ‹ ---  ðŸŒ‹ \n",
      "â™‚ ---  â™‚ \n",
      "ðŸŽ¤ ---  ðŸŽ¤ \n",
      "ðŸ–• ---  ðŸ–• \n",
      "â¤µ ---  â¤µ \n",
      "ðŸ§¿ ---  ðŸ§¿ \n",
      "ðŸ“Œ ---  ðŸ“Œ \n",
      "ðŸ§¸ ---  ðŸ§¸ \n",
      "ðŸš‚ ---  ðŸš‚ \n",
      "ðŸ’ ---  ðŸ’ \n"
     ]
    }
   ],
   "source": [
    "# Try Split word\n",
    "def split_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "            chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "            temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(split_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 19538 | Known words: 8791\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion\n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "\n",
    "def convert_leet_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = convert_leet(word)\n",
    "        if (new_word!=word):\n",
    "            if (len(word)>2) and (new_word in local_vocab):\n",
    "                temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_leet_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 19548 | Known words: 8791\n",
      "43000 --- @NUM[43000.0]\n",
      "47716 --- @NUM[47716.0]\n",
      "458 --- @NUM[458.0]\n",
      "408 --- @NUM[408.0]\n",
      "564 --- @NUM[564.0]\n",
      "599 --- @NUM[599.0]\n",
      "100awayfrom --- @NUM[100.0] awayfrom\n",
      "614 --- @NUM[614.0]\n",
      "012088 --- @NUM[12088.0]\n",
      "726 --- @NUM[726.0]\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 19547 | Known words: 8791\n",
      "coinbase --- @coinbase\n",
      "paypal --- @paypal\n",
      "crypto --- #cryptocurrency\n",
      "cryptocurrency --- #cryptocurrency\n",
      "hodl --- #hodl\n",
      "blockchain --- #blockchain\n",
      "bitstamp --- @bitstamp\n",
      "binance --- @binance\n",
      "altcoins --- #altcoins\n",
      "bitcoins --- $bitcoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 19538 | Known words: 8791\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "@bitstamp --- @USR[bitstamp]\n",
      "@binance --- @USR[binance]\n",
      "#hodl --- @HTAG[hodl]\n",
      "@paypal --- @USR[paypal]\n",
      "@coinbase --- @USR[coinbase]\n",
      "#altcoins --- @HTAG[altcoins]\n",
      "#cryptocurrency --- @HTAG[cryptocurrency]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 19538 | Known words: 8791\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 19231 | Known words: 8868\n"
     ]
    }
   ],
   "source": [
    "# Remove placeholders\n",
    "def remove_placeholders(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (not check_replace(k) and k.startswith(WPLACEHOLDER))]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_placeholders)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 19210 | Known words: 8868\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "def search_multiple_form(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "    temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(search_multiple_form)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 19231 | Known words: 8868\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 19231 | Known words: 8868\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 19226 | Known words: 8868\n",
      "usdt --- $tether\n",
      "xlm --- $stellar\n",
      "aave --- $aave\n",
      "dgb --- $digibyte\n",
      "yfi --- $yearn_finance\n",
      "ethereum --- $ethereum\n",
      "trx --- $tron\n",
      "egld --- $elrond_egld\n",
      "ves --- $ves\n",
      "algo --- $algorand\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 19210 | Known words: 8868\n",
      "$xrp --- @CURR[xrp]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "$tron --- @CURR[tron]\n",
      "$digibyte --- @CURR[digibyte]\n",
      "$tether --- @CURR[tether]\n",
      "$litecoin --- @CURR[litecoin]\n",
      "$dogecoin --- @CURR[dogecoin]\n",
      "$ves --- @CURR[ves]\n",
      "$aave --- @CURR[aave]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 19210 | Known words: 8868\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: 0.04115000000000002\n",
      "########## Step - Language datection:\n",
      "Unknown words: 17597 | Known words: 8718\n"
     ]
    }
   ],
   "source": [
    "# Cut away non english tweets\n",
    "model = fasttext.load_model('../../data/kaggle/lid.176.ftz')\n",
    "\n",
    "def langcheck(item, min_confidence=0.2):\n",
    "    text = ' '.join([w for w in item.split() if not w.startswith('@')])\n",
    "    if len(text) < 3:\n",
    "        return True\n",
    "    results = dict(zip(*model.predict(text, k=2)))\n",
    "    return results.get('__label__en', 0) > min_confidence\n",
    "\n",
    "mask = texts.parallel_map(langcheck)\n",
    "if verbose: print(f'Deleted: {1 - sum(mask)/len(texts)}')\n",
    "texts = texts[mask]\n",
    "data = data[mask]\n",
    "if verbose: print('#' * 10, 'Step - Language datection:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "                       _id  \\\n0      1360142875330232324   \n1      1360140112861003776   \n2      1360137307047694337   \n4      1360132401142366210   \n5      1360131434158170113   \n...                    ...   \n19995  1357792968455946242   \n19996  1357792933982928896   \n19997  1357792930359107588   \n19998  1357792864005095424   \n19999  1357792837870510083   \n\n                                                                                                                                                                                                          text  \n0      when the top u . s . central banker gets photobombed by @CURR[bitcoin] . ðŸ‘‰ ðŸ‘€ @CURR[bitcoin] @CURR[bitcoin] @HTAG[cryptocurrency] @HTAG[cryptocurrency] @CURR[ethereum] @HTAG[ripple] @HTAG[link] @CU...  \n1      best am arriving with exciting features @HTAG[bsc] @USR[binance] @CURR[bitcoin] @HTAG[binancesmartchain] @HTAG[defi] @HTAG[definews] @HTAG[stafi] @HTAG[cake] @HTAG[pancakeswap] @HTAG[paraswap] @HT...  \n2      to keep its ultra bullish run intact , $ $elrond_egld _ $elrond_egld bulls need to keep $ $elrond_egld _ $elrond_egld / @CURR[tether] daily above @NUM[148.0] dollar . reclaiming @NUM[174.0] dollar...  \n4      next coin that goes @NUM[100.0] percent . . . buckle up . . . @CURR[tezos] @CURR[tezos] @CURR[tezos] look @ my calls from last 2 weeks @CURR[iota] @HTAG[coti] @CURR[tezos] will move hard incoming ...  \n5                        its gonna be huge ! ðŸš€ ðŸ˜ ðŸ‘‘ @HTAG[fetch_ai] ðŸ‘‘ @CURR[xrp] @CURR[vechain] @CURR[chainlink] @CURR[cardano] @CURR[algorand] @HTAG[altcoins] @HTAG[artificialintelligence] @HTAG[blockchain]  \n...                                                                                                                                                                                                        ...  \n19995                                                                                                                                                                             cash is trash @CURR[bitcoin]  \n19996                                                                                               global central bank efforts to limit u . s . dollars decline raises specter of currency war @CURR[bitcoin]  \n19997                                                                                                                                       what if @CURR[bitcoin] is a social experiment ? well , money was .  \n19998                                                                                                       @CURR[bitcoin] btw that was pre close ny - cme friday dump . pl are closing positions b4 weekend .  \n19999                                                                                                                                           nigeria is fucked and pregnant with stupidity . @CURR[bitcoin]  \n\n[19177 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1360142875330232324</td>\n      <td>when the top u . s . central banker gets photobombed by @CURR[bitcoin] . ðŸ‘‰ ðŸ‘€ @CURR[bitcoin] @CURR[bitcoin] @HTAG[cryptocurrency] @HTAG[cryptocurrency] @CURR[ethereum] @HTAG[ripple] @HTAG[link] @CU...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1360140112861003776</td>\n      <td>best am arriving with exciting features @HTAG[bsc] @USR[binance] @CURR[bitcoin] @HTAG[binancesmartchain] @HTAG[defi] @HTAG[definews] @HTAG[stafi] @HTAG[cake] @HTAG[pancakeswap] @HTAG[paraswap] @HT...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1360137307047694337</td>\n      <td>to keep its ultra bullish run intact , $ $elrond_egld _ $elrond_egld bulls need to keep $ $elrond_egld _ $elrond_egld / @CURR[tether] daily above @NUM[148.0] dollar . reclaiming @NUM[174.0] dollar...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1360132401142366210</td>\n      <td>next coin that goes @NUM[100.0] percent . . . buckle up . . . @CURR[tezos] @CURR[tezos] @CURR[tezos] look @ my calls from last 2 weeks @CURR[iota] @HTAG[coti] @CURR[tezos] will move hard incoming ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1360131434158170113</td>\n      <td>its gonna be huge ! ðŸš€ ðŸ˜ ðŸ‘‘ @HTAG[fetch_ai] ðŸ‘‘ @CURR[xrp] @CURR[vechain] @CURR[chainlink] @CURR[cardano] @CURR[algorand] @HTAG[altcoins] @HTAG[artificialintelligence] @HTAG[blockchain]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>1357792968455946242</td>\n      <td>cash is trash @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>1357792933982928896</td>\n      <td>global central bank efforts to limit u . s . dollars decline raises specter of currency war @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>1357792930359107588</td>\n      <td>what if @CURR[bitcoin] is a social experiment ? well , money was .</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>1357792864005095424</td>\n      <td>@CURR[bitcoin] btw that was pre close ny - cme friday dump . pl are closing positions b4 weekend .</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>1357792837870510083</td>\n      <td>nigeria is fucked and pregnant with stupidity . @CURR[bitcoin]</td>\n    </tr>\n  </tbody>\n</table>\n<p>19177 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = texts\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO:\n",
    "* numbers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}